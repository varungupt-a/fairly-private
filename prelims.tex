% !TEX root = main.tex

\section{Preliminaries}
Let $\X$ be our data universe consisting of elements $z=(x,a,y)$
 where $x$ is the features, $a$ the protected (binary) attribute, and
 $y$ the binary response. Note that $x$ may have arbitrary correlation
 with $a$, including containing a copy of it. $\A(x)$ is the
 probability distribution over outputs of a randomized algorithm $A$ on
 input $x$.

% First we consider the interaction of differential privacy and fairness in the full distribution access setting in which we show the impossibility of achieving both.

% We consider data universe $\X$, where we have access to a joint probability distribution $D = (X, A, Y)$ over $\X$, where $X$ are the features, $A$ sensitive attributes, and $Y$ true labels, respectively. Distribution $D$ will represent a database in $\X$, as we specify below. Let $d_i = (x_i, a_i, y_i)$ be instances sampled from the distribution $D$. Note that $x_i$ may have arbitrary correlation with $a_i$, including containing a copy of it.
%  \\ \\
% In the discrete setting:

% We consider data universe $\X$. We have a finite sample database $Z$ with $n$ entries, with labeled examples $z_i = (x_i, a_i, y_i)$, drawn from an abitrary distribution over $\X$. In this context, we define $\zya = \{z \in Z | y_i =y, a_i = a\}$.


\subsection{Differential Privacy}

For our results, we use two notions of a database:

\begin{enumerate}
	\item a distribution $D$ over $\X$;
	\item a finite sample, vector $Z = (z_1, ..., z_n)$ drawn i.i.d. from a distribution $D$ over $\X$.
\end{enumerate}
For each notion, there is an accompanying notion of a neighboring
database. We present them here in respective order.

\begin{defn}
  ($\zt$-closeness) \cite{2dplimits}. (1) Random variables $D$ and $D'$ taking values in
  $\X$ are $\zt$-close if the statistical distance between their
  distributions is at most $\zt$, i.e.,

	$$ ||D-D'||_{\textit{SD}} :=\frac{1}{2}\sum_{d\in \mathcal{X}}
  |\Pr[D=d] - \Pr[D'=d]| \leq \zeta.$$
\end{defn}

\begin{defn} \cite{Kasiviswanathan:2011:WLP:2078965.2078976}.
  (2) Samples $Z$ and $Z'$ are neighboring if $z_i \neq z_i'$ for
  exactly one $i \in [n]$ (i.e. the Hamming distance between $Z$ and
  $Z'$ is 1).
\end{defn}
Alternatively we could define a finite sample as a multi-set, and use
the symmetric distance instead of the Hamming metric to measure
distance. Note that Definition 2.1 is a generalization of Definition
2.2.

For each of these notions, the definition of differential privacy
remains the same. That is, a randomized algorithm is private if
neighboring databases induce close distributions on its outcomes:

\begin{defn}
  ($\eps$-differential privacy). \cite{dpbook} A randomized algorithm
    $\A$ is $\eps$-differentially private if for all pairs of
    neighboring databases $D,D'$ and for all sets $\S$ of outputs,
    $$\Pr[\A(D)\in \S] \leq \exp(\eps)\Pr[\A(D')\in \S].$$
\end{defn}
\dk{need to talk about composition/post-processing theorems and/or exponential mechanism?}

\begin{theorem} Adaptive composition
\end{theorem}

\subsection{Exact Fairness}

In this setting our database notion is (1): a distribution $D$ over
$X$. This corresponds with the full distributional access setting of
\cite{hardteqop} where we have access to the joint probability
distribution $D = (X, A, Y)$ over $\X$, where $X$ are the features, $A$
sensitive attributes, and $Y$ true labels, respectively. Hence we state
our notion of exact fairness in these terms:

\begin{defn}
  (Equal Opportunity) \cite{hardteqop}. We say that a binary predictor $h$ satisfies
    equal opportunity with respect to $A$ and $Y$ if
    $$\Pr[h = 1 | Y=1, A=1] = \Pr[h = 1 | Y=1, A=0].$$
    
   	(Equality of group conditional true positive rates) We will denote these probabilities as,
   	$$\gamma_{ya} := \Pr[h = 1 | Y=y, A=a]$$
\end{defn}

Note that this is the weaker of the two definitions of fairness in
\citet{hardteqop} however, it is sufficient to prove impossibility for this
notion since violating this notion implies violating the stronger
notion, equal odds. These measures of fairness belong to a broader
class of fairness constraints that is, constraints on functions of the
confusion matrix of a classifier .

For precision, we also assume throughout the paper that $\Pr[A=a, Y=y] >0$ for $a,y \in \{0,1\}$.

\subsection{Approximate Fairness}
We now turn to the approximate fairness setting with and define
analogous definitions from the exact fairness setting. Here our notion
of a database is (2) i.e. we have access to a finite sample $Z$
consisting of entries drawn i.i.d. from an arbitrary distribution $D$
over $\X$.

For ease of notation we define $\zya := \{z_i \in Z | y_i = y, a_i = a \}$.

We also have an analogous definition for the group-conditional true
positive rates: \dk {clarify summation as such?: $\sum_{z_i \in \zya} h(x_i)$}
$$\gamma_{ya}^Z(h) = \frac{1}{|\zya|} \sum_{\zya} h(x).$$

(This is some abuse of notation but the distinction between finite sample and full distribution settings will be clear from the context.)

\begin{defn}
	($\alpha$-discrimination) \cite{woodworthFollowUp}. We say that a binary
    predictor $h$ is $\alpha$-discriminatory with respect to a binary
    protected attribute $A$ on the population or on a sample $Z$ if,
    respectively:

$$\Gamma(h) := \max_{y\in \{0,1\}}|\g{y0} - \g{y1}| \leq \alpha ~~$$ {or}
$$\Gamma^Z(h) := \max_{y\in \{0,1\}}|\gz{y0} - \gz{y1}| \leq \alpha.$$

\end{defn}
When $\alpha = 0$ we are in the exact fairness setting. Note that by
\citet{woodworthFollowUp} the sample fairness measure $\Gamma^Z$ converges to the
population measure $\Gamma$ when $n$ is large. For the
analog of Definition 2.4 (eq. opp) where we only consider true positive
rates, the definition reduces to:

$$\Gamma^Z(h) := |\gz{10} - \gz{11}| \leq \alpha.$$

In the remainder of this paper, variable $\Gamma$ refers to this
fairness measure.

\dk {talk about motivation for approx fairness because of finite sample
(cannot achieve exact) and now also because of impossibility with privacy.}

\subsection{Preliminaries from learning theory}
\dk {do we need to factor a weight $\lambda$ into the new error
function? ie. $\ell + \lambda \Gamma$}
\dk {very not sure how to write this section, help?}
A concept is a function that labels examples taken from the domain $X$
by the elements of the range $Y$. A \emph{concept class} is a set of
concepts. The domain and range in $\cC$ are ensembles $X= \{X_d\}_{d\in
\N}$, $Y= \{Y_d\}_{d\in \N}$ where the representation size of the
elements in $X_d,Y_d$ is at most $d$. Since we focus on binary
classification problems, $d$ measures the size of examples in $X_d$.
When the size parameter is clear from the context or not important, we
omit the subscript $d$.

In our particular setting, our distribution of labeled examples is
arbitrary on $X_d \times Y_d$ and hence we consider the agnostic
setting that removes the realizability assumption. The goal of a
learner then is to output a hypothesis $h \in \H$ whose error with
respect to the distribution is close to the optimal possible by a
function from $\cC$. The misclassification error of $h$ on $D$ is
defined as:

$$err(h) = \Pr_{(x,y) \tilde D}[h(x) \neq y].$$

In our setting we consider the agnostic learnability setting.
\dk {not sure if following def is correct (wrt. concept class $\cC_d$ do
we consider $\cC$ in the agnostic setting?)}
\begin{defn}
	(Agnostic PAC Learning) A concept class $\cC$ over $\X$ is PAC
  learnable using hypothesis class $\H$ if there exists an algorithm
  $\A$ and a polynomial $poly(\cdot,\cdot)$ such that for all $d \in
  \mathbb{N}$, all concepts $c \in \mathcal{C}_d$, all distributions
  $D$ on $\X$ and all $\alpha,\beta \in (0,1/2)$, given inputs
  $\alpha,\beta,$ and $Z = (z_1, ... z_n)$, where n = $poly(d,
  1/\alpha, \log(1/\beta))$, $z_i = (x_i, a_i, y_i)$ drawn i.i.d. from
  D for $i \in [n]$, algorithm $\A$ outpus a hypothesis $h \in \H$
  satisfying
	$$\Pr[err(h) \leq OPT + \alpha] \geq 1-\beta.$$

\end{defn}
\dk {insert note about distribution-free learning}
\subsection{Private and Approximately Fair Agnostic Learning}
We define private and approximately fair PAC learners as algorithms that satisfy the definitions of differential privacy, approximate fairness (with high probability), and PAC learning.
\begin{defn}
	(Private and Approximately Fair Agnostic PAC Learning)
	Let $d, \alpha, \beta$ be as in Definition 2.6 (agnostic PAC
  learnigng) and $\eps > 0$. Concept class $\cC$ is (inefficiently)
  privately and approximately fair agnostically learnable using
  hypothesis class $\H$ if there exists an algorithm $\A$ that takes
  inputs $\eps, \alpha, \beta, Z$, where $|Z|=n$ is polynomial in
  $1/\eps, d, 1/\alpha, \log(1/\beta)$ and satisfies
	\begin{enumerate}
		\item {[}Fairness{]} Algorithm $\A$ satisfies $\Pr[\Gamma(h) \leq
    OPT + \alpha] \geq 1-\beta$;
		\item {[}Privacy{]} For all $\eps>0$, algorithm $\A(\eps, \cdot, \cdot, \cdot)$ is $\eps$-differentially private;
		\item {[}Utility{]} Algorithm $\A$ agnostically PAC learns $\cC$ using $H$.
	\end{enumerate}
\end{defn}
