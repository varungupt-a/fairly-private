% !TEX root = main.tex

\section{Approximate fairness with differentially privacy}



We now turn to the finite sample setting where we release a hypothesis
that minimizes the training error. Our goal is now approximate
fairness. We use the exponential mechanism in the same way as it is
used in private PAC learning. Defining the sample as $Z$, $|Z| = n$ and
$\Gamma^{Z}$ as our in-sample fairness measure, we give the algorithm:
$$\mathcal{A}^\eps : \text{Output hypothesis }h \in \mathcal{H} \text{
with probability proportional to }$$
\begin{align}
\exp(-\frac{\eps \cdot u(Z,h)}{2\Delta u})
\end{align}

where

$$u(Z,h) = ||(\Gamma^Z(h), \ell^Z(h)||_{1}$$.
$$\Delta u(Z,h) = ||(\Delta\ell,\Delta{\Gamma})||_1 \approx O(||1/n,1/|\z{10}|+1/|Z_{11}|||_1)$$

$$\ell^Z(h) = \frac{1}{n} \sum_{(x,y) \in Z}\Pr[h(x) \neq y]$$



This algorithm is the exponential mechanism in McSherry and Talwar, and so it is differentially private.

\begin{lemma}
  The algorithm $\A_\eps$ is $\eps$-differentially private.
\end{lemma}
\vg{Need proof?}

Note that except for when $|\H|$ is polynomial, the exponential
mechanism does not necessarily yield a polynomial time algorithm.


\begin{theorem}
	(Generic private fair learner) For all $d \in \mathbb{N}$, any
  concept class $\cC_d$ whose cardinality is at most
  $\exp(\text{poly}(d))$ is privately and approximately fairly
  agnostically learnable using $\H_d = \cC_d$. More precisely, the
  learner uses $n = ..$ labeled examples from $D$, where $\eps,
  \alpha$, and $\beta$ are parameters of the private learning.
  \dk {need to be sure of constants before calculating n. Will be poly for sure}
\end{theorem}

\begin{proof}
	Let $\A_{\eps}$ be as defined above. The privacy condition is
  satisfied by Lemma.

	Now we show that the utility condition is also satisfied. Let the
  event $E = \{\A_{\eps} = h \text{ with } u(h) > OPT + \alpha\}$. We
  need that $\Pr[E] \leq \beta$. We define the utility of $h$ as

	$$u(Z,h) = ||(\Gamma^Z(h), \ell^Z(h)||_{1}$$.
\dk {introduce this before proof and have a discussion about it? OR discussion after proof about utility and implication of $\alpha$ accuracy/fairness whp.}

	Recall that $Z$ is the sample drawn i.i.d. from a distribution $D$.
  By Chernoff-Hoeffding bounds (insert appendix ref. see below proof
  for now),

	$$\Pr[|u(Z,h) - u(D,h)| \geq \rho] \leq 4\exp(\frac{-\rho^2n}{2})$$

	for all hypotheses $h \in \H_d$. Hence,

	$$\Pr[|u(Z,h) - u(D,h)| \geq \rho \text{ for some } h \in \H_d] \leq 4|\H_d|\exp(\frac{-\rho^2n}{2})$$

	\dk{need following proof? relatively similar to kasiviswanathan what
  can we learn privately, except we have different utility and sensitivity}
	Now we analyze $\A_\eps(Z)$ conditioned on the event that for all
  $h\in \H_d$, $|u(Z,h) - u(D,h)| < \rho$. For every $h \in \H_d$, $\Pr[\A_\eps(Z) = h]$ is

	$$\frac{\exp(-\frac{\eps}{2\Delta u} \cdot
  u(Z,h))}{\sum_{h'\in\H_d}\exp(-\frac{\eps}{2\Delta u} \cdot u(Z,h'))}
  \leq \frac{\exp(-\frac{\eps}{2\Delta u} \cdot
  u(Z,h))}{\max_{h'\in\H_d}\exp(-\frac{\eps}{2\Delta u} \cdot u(Z,h'))} $$
	$$= \exp(-\frac{\eps}{2\Delta u}(u(Z,h) - \min_{h'\in\H_d}u(Z,h')))$$
	$$\leq \exp(-\frac{\eps}{2\Delta u}(u(Z,h) - (OPT + \rho)))$$

	Hence the probability that $\A_\eps(Z)$ outputs a hypothesis $h \in
  \H_d$ such that $u(Z,h) > OPT + 2\rho$ is at most
  $|\H_d|\exp(-\frac{\eps\cdot\rho}{2\Delta u})$

	Setting $\rho = \alpha/3$. If $u(D,h) \geq OPT + \alpha$ then
  $|u(D,h) - u(Z,h)| \geq \alpha/3$ or $u(Z,h) \geq OPT + 2\alpha/3$.
  Hence

	$$\Pr[E] \leq |\H_d|(4\exp(\frac{-\alpha^2n}{18}) + \exp(-\frac{\eps\cdot\alpha}{6\Delta u})) \leq \beta$$.

	Where the inequality holds for $n \geq $.
\end{proof}
\begin{theorem}[Real-valued Additive Chernoff-Hoeffding Bound.]
	Let $X_1,...,X_d$ be i.i.d. random variables with $\mathbb{E}[X_i] = \mu$ and $a \leq X_i \leq b$ for all $i$. Then for every $\rho > 0$,
$$Pr[|\frac{\sum_i X_i}{n} - \mu| > \rho] \leq 2\exp(\frac{-2\rho^2n}{(b-a)^2})$$
\end{theorem}

\begin{lemma}[srebro ref]
	For $\delta \in (0,1/2)$ and a binary predictor $h$,
	$$\Pr[|\Gamma(h) - \Gamma^Z(h)|>\rho ] \leq 16 \exp(\frac{1}{4}\rho^2n
	 \min_{ya}{P_{ya}})$$
\end{lemma}

$$\Pr[|u(Z,h) - u(D,h)| > \rho] \leq 4\exp(\frac{-2\rho^2n}{9})$$

\dk {Note: can do extension into other definitions of fairness (with constant factor).
	Can also extend into a different norm for $u(Z,h)$ if we have a
  concentration of measure theorem for the different norm. There are
  also random matrix concentration bounds (can do concentration for all
  functions of the confusion matrix?)}
