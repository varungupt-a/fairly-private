% !TEX root = main.tex

\section{Achieving exact fairness and differential privacy is impossible}
We start with the full distributional access setting (which can be translated to the finite sample setting later). Let $\mathcal{X}$ be the data universe with elements $d_i = (x_i,a_i,y_i) \in \mathcal{X}$ where each entry consists respectively of the response, features, and protected attribute. Note that $x_i$ may have arbitrary correlation with $a_i$, including containing a copy of it.


\dk {actually we could do an impossibility result for
	any hard threshold for $\Gamma$ in any setting. Hence motivating approx
	fairness with high probability}

\begin{defn}
	For ease of notation we define
	$$\X_{ya} := \{d_i \in \X | y_i = y, a_i = a \}.$$
\end{defn}
\begin{defn}
	(non-trivial hypothesis class). We say that a hypothesis class $\H$ is  \emph{non-trivial} if there exists $d_1, d_0 \in \mathcal{X}$ such that for some $h \in \H$, $h(x_1) > h(x_0)$.
\end{defn}
\begin{lemma}Let $\mathcal{H}$ be a non-trivial hypothesis class. Then
  releasing an exactly fair hypothesis $h\in \mathcal{H}$ in a
  differentially private manner is impossible.
\end{lemma}
\dk{  TODO: general proof of impossibility given a
`reasonable' fairness constraint on the confusion matrix? Seems like it
would be difficult since need to construct neighboring databases. }

\begin{proof}
	We will construct distributions $D, D'$ that are $\zt$-close and show impossibility. Suppose that the fair (Definition 2.4) hypothesis $h$ has a non-zero probability of being released by algorithm $\mathcal{A}$ on input of $D$. Let $d(\cdot)$ be the measure as
  induced by
  distribution $D$.

  WLOG let $d_1, d_0 \in \X_{11}$ such that $h(x_1) >
  h(x_0)$. Assume $\X_{11}\in supp(D) \text{ and } supp(D')$.
  Let $0 < \zt < \min_{i,D,D'}(d_{D}(d_i))$. Let $D'$ be a database with $\zt$
  more probability mass of $d_1$ and $\zt$ less mass of $d_0$. Then
  recalling that $d_i = (x_i, a_i,y_i)$:

$$\gamma_{11}^{D'} = \frac{1}{d(\X_{11})}\int_{\X_{11}}h(x)dx +
\frac{\zt}{d(\X_{11})} h(x_1) - \frac{\zt}{d(\X_{11})} h(x_0) $$
\begin{equation}
=\gamma_{11}^{D} + \frac{\zt}{d(\X_{11})} (h(x_1) -
 h(x_0))>\gamma_{11}^{D}
\end{equation}
where the last inequality is due to the assumption that $h(x_1) >
h(x_0)$ and $\zt >0$.

Since we have not modified the distributions over $\X_{10}$,
$$\gamma_{11}^D = \gamma_{10}^{D} = \gamma_{10}^{D'} \neq  \gamma_{11}^{D'}$$.

Where the first equality is due to the assumption that $h$ is fair on $D$,
 the second equality from the fact that $D=D'$ over $\X_{10}$ and the third
 equality by (1). Hence hypothesis $h$ is unfair (definition
 2.4) when evaluated on distribution $D'$
 and cannot be released by the exactly fair algorithm $\A$ on input of
 database $D'$. Hence,

$$\Pr\{\mathcal{A}(D) = h\} \not\leq \exp(\eps)\Pr\{\mathcal{A}(D') = h\} = 0$$.

which violates the definition of differential privacy (definition 2.3)
\end{proof}

This result also applies to more general notions of statistical fairness. Definition 2.4, Equal Opportunity, is a strictly weaker notion of fairness than Equal Odds also defined in \citet{hardteqop}. In other words, Equal Opportunity is a necessary condition for Equal Odds.

We can also use the same proof method to show impossibility of fairness and privacy
when we consider notions of approximate fairness such as disparate impact and mean
difference scores. Approximate in these notions of fairness meaning that the
resulting classifier (or anything released by the randomized algorithm) satisfies some hard threshold on the `level of discrimination' on the distribution.
One needs to construct a neighboring distribution where when considering a hypothesis $h\in \H$, $h$ is fair on one distribution but not the other.


\begin{lemma}
  $(\eps, \zt)$-DP and fairness is impossible. \vg{need proof?}
\end{lemma}




\begin{lemma} $\Gamma(h) = |\Pr\{h(x) = 1 | y=1, a =0\} - \Pr\{h(x) = 1 | y = 1, a = 1\}|$ has sensitivity $\max(\frac{1}{\z{10}},\frac{1}{|Z_{11}|}, \frac{\gamma_{10}}{|\z{10}|-1} + \frac{\gamma_{11}}{|Z_{11}|+1}, \frac{\gamma_{10}}{|\z{10}|+1} + \frac{\gamma_{11}}{|Z_{11}|-1})$
\end{lemma}

\vg{Proof 1 is in the finite sample setting and Proof 2 is in the
distributional setting, not sure which of the two to keep. Finite
sample is easier/more applicable but the distributional is
general/converted easily to finite. We're leaning towards keeping
finite sample but aren't sure }

\dk{this proof seems like appendix material}

\dk{should we use the term "database" or "sample" for the finite sample setting?}
\begin{proof}
We examine two cases for neighboring databases $Z, Z'$:

Case 1: The difference in databases is in an entry within a subgroup
(i.e. within $Z_{1a}$ and $Z_{1a}'$ for $a \in \{0, 1\}$). WLOG let $a
= 0$. Let the differing entries be called $z \in \z{10}$ and $z' \in
Z'_{10}$. Then, we have

$$\gamma^{Z'}_{11} = \gamma^Z_{11}$$

but

$$\gamma_{10}^Z(h) = \frac{1}{|\z{10}|} \sum_{\z{10}} h(x,0)$$.
$$\gamma_{10}^{Z'}(h) = \frac{1}{|\z{10}|} (\sum_{z \in \z{10} \cup \z{10}'} h(x, 0) + h(x', 0)).$$

Then,

$$ \Gamma^{Z'}(h) = \gamma_{10}^{Z'}(h) - \gamma_{10}^{Z'}(h) $$
$$ = \gamma_{10}^{Z'}(h) - \gz{11} $$
$$ \leq \Gamma^{Z}(h) + \frac{1}{|\z{10}|} $$


Case 2: The neighboring databases are different in that an entry is added to one subgroup and another entry is removed from the other subgroup. Thus WLOG $Z' =(Z_{11}\setminus
\{z_1\} )\cup( \z{10}\setminus \{z_0\})$. Let $\gamma_{11}^{Z}(h)
<\gamma_{10}^{Z}(h)$ then

$$\gamma_{11}^{Z'}(h) = \frac{1}{|Z'_{1a}|} \sum_{Z'_{1a}} h(x)$$
$$= \frac{1}{|Z'_{11}|} (\sum_{Z_{11}} h(x)-h(x_1)) = \frac{1}{|Z_{11}|-1} (\sum_{Z_{11}} h(x)-h(x_1))$$
Similarly,
$$\gamma_{10}^{Z'}(h) = \frac{1}{|Z'_{10}|} \sum_{Z'_{10}} h(x)$$
$$ = \frac{1}{|Z'_{10}|} (\sum_{\z{10}} h(x)+h(z_0))= \frac{1}{|\z{10}|+1} (\sum_{\z{10}} h(x)+h(z_0))$$

Hence with notation $ \Gamma^Z = \Gamma^Z(h)$ for clarity, $$|\Gamma^{Z'}- \Gamma^{Z}| = (\gamma_{10}^{Z'} - \gamma_{11}^{Z'}) - (\gamma_{10}^{Z} - \gamma_{11}^{Z})$$


$$=(\gamma_{10}^{Z'}- \gamma_{10}^{Z}) + (\gamma_{11}^{Z} -\gamma_{11}^{Z'})$$
Let $\zt_{10}= \gamma_{10}^{Z'}- \gamma_{10}^{Z}$ and  $\zt_{11} = \gamma_{11}^{Z} -\gamma_{11}^{Z'}$

$$\zt_{10} = \frac{1}{|\z{10}|+1} (\sum_{\z{10}} h(x)+h(z_0)) - \gamma_{10}^{Z}$$
$$\zt_{11} = \gamma_{11}^{Z}- \frac{1}{|Z_{11}|-1} (\sum_{Z_{11}} h(x)-h(x_1))$$
\dk {insert rest of proof, easy algebra}

$$|\Gamma^{Z'}- \Gamma^{Z}| = \zt_{10} + \zt_{11}= \frac{\gamma_{10}}{|\z{10}|-1} + \frac{\gamma_{11}}{|Z_{11}|+1} $$
\end{proof}

\begin{proof}
Construct $\zt$-close neighboring databases $D$ and $D'$ such that

$$\Pr[D'=d_{10}] - \Pr[D=d_{10}] = \zt$$
$$\Pr[D=d_{11}] - \Pr[D'=d_{11}] = \zt$$



and assume that $d_0, d_1 \in \text{supp}(D')$ and $\in \text{supp}(D)$. Let $d_{10} \in D_{10}$ and $d_{11} \in D_{11}$ and WLOG $\gamma_{10}^Z(h) > \gamma_{11}^Z(h)$ then,

$$\Gamma^{D'}(h) = \gamma_{10}^{D'}(h) - \gamma_{11}^{D'}(h)$$
$$= \int_{D'_{10}}h(x)dx - \int_{D'_{11}}h(x)dx$$
$$= \int_{D_{10}}h(x)dx + \frac{\zt}{dD_{10}}h(x_{10}) - \int_{D'_{11}}h(x)dx - \frac{\zt}{dD_{11}}h(x_{11})$$
$$\leq \Gamma^{D}(h) + \frac{\zt}{dD_{10}} + \frac{\zt}{dD_{11}}$$

where $dD_{1a}$ is the measure of the set $D_{1a},$ for $a \in \{0,1\}$. Hence,
$$\zt\Gamma = \frac{\zt}{dD_{10}} + \frac{\zt}{dD_{11}}.$$
\end{proof}

\dk{Q: more elegant/complete proof? right now it seems we are doing by cases. ie. $\zt$ close changes $\gamma_{ya}(h)$ by at most ... therefore ..}
