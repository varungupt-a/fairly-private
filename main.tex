\documentclass[format = sigconf]{acmart}
\usepackage{amsmath,amsthm,amssymb,amsfonts}
\usepackage{mathtools}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{fancyhdr}
\usepackage{hyperref}
\usepackage{natbib}
\usepackage{bbm}
\usepackage{enumitem}
\newcommand{\dk}[1]{\textcolor{red}{[DK: #1]}}
\newcommand{\vg}[1]{\textcolor{blue}{[VG: #1]}}


\newcommand\tab[1][0.5cm]{\hspace*{#1}}
\newcommand{\I}{\mathcal{I}}
\newcommand{\M}{\mathcal{M}}
\newcommand{\A}{\mathcal{A}}
\renewcommand{\H}{\mathcal{H}}
\renewcommand{\C}{\mathcal{C}}
\newcommand{\X}{\mathcal{X}}
\renewcommand{\S}{\mathcal{S}}


\newcommand{\1}{\mathbbm{1}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}


\newcommand{\eps}{\epsilon}
\newcommand{\zt}{\zeta}
\newcommand{\zya}{\Z_{ya}}
\iffalse
\newcommand{\Z}[1]{Z_{#1}}
\newcommand{\D}[1]{D_{#1}}
\newcommand{\D}[1]{D_{#1}}

\newcommand{\g10}[1]{\gamma_{10}^Z(h)}
\newcommand{\g11}{\gamma_{11}^Z(h)}

\newcommand{\g10'}{\gamma_{10}^{Z'}(h)}
\newcommand{\g11'}{\gamma_{11}^{Z'}(h)}

\newcommand{\GZ}{\Gamma^{Z'}(h)}
\newcommand{\GZ'}{\Gamma^{Z'}(h)}
\fi
\newcommand{\lzh}{\ell^{Z}(h)}


%
\theoremstyle{definition}
\newtheorem{defn}{Definition}[section]

%
\newenvironment{claim}[1]{\par\noindent\underline{Claim:}\space#1}{}
\newenvironment{claimproof}[1]{\par\noindent\underline{Proof:}\space#1}

\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,
    urlcolor=blue,
}

\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}
\DeclarePairedDelimiter{\floor}{\lfloor}{\rfloor}



\newenvironment{problem}[2][Problem]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
%If you want to title your bold things something different just make another thing exactly like this but replace "problem" with the name of the thing you want, like theorem or lemma or whatever

\begin{document}

%\renewcommand{\qedsymbol}{\filledbox}
%Good resources for looking up how to do stuff:
%Binary operators: http://www.access2science.com/latex/Binary.html
%General help: http://en.wikibooks.org/wiki/LaTeX/Mathematics
%Or just google stuff

\title{Privacy $\cap$ Fairness}
{\author{Rachel Cummings, Varun Gupta, Dhamma Kimpara, Jamie Morgenstern}}
\maketitle


\section{Introduction}
\subsection{Related Work}
\subsection{Our Contributions}
We present two contributions to the study of the intersection of differential privacy and fairness in machine learning. First, we show that it is impossible to achieve both differential privacy and exact fairness, as we will define below. We then consider a notion of approximate fairness and show that a private learner is able to accurately achieve fairness with reasonable sample complexity. \vg{Finally, we present a polynomial time algorithm to achieve the differentially private and approximately fair scheme outlined above and show its empirical performance on certain datasets.}

\section{Preliminaries}
We consider the interaction of differential privacy and fairness and look at two settings, discrete and continuous. \\ \\
We begin with notation of continuous setting:

We consider data universe $\X$, where we have access to a joint probability distribution $D = (X, A, Y)$ over $\X$, where $X$ are the features, $A$ sensitive attributes, and $Y$ true labels, respectively. Distribution $D$ will represent a database in $\X$, as we specify below. Let $d_i = (x_i, a_i, y_i)$ be instances sampled from the distribution $D$. Note that $x_i$ may have arbitrary correlation with $a_i$, including containing a copy of it.
 \\ \\
In the discrete setting:

We consider data universe $\X$. We have a finite sample database $Z$ with $n$ entries, with labeled examples $z_i = (x_i, a_i, y_i)$, drawn from an abitrary distribution over $\X$. In this context, we define $\zya = \{z \in Z | y_i =y, a_i = a\}$.
\subsection{Differential Privacy}
We consider differential privacy in two settings, discrete and continuous.
We first define the neighboring relation for differential privacy in the continuous setting.

\begin{defn}
	($\zt$-closeness). Databases  $D$ and $D'$ (distributions over $\mathcal{X}$) are $\zt$-close if the statistical distance between their distributions is at most $\zt$, i.e.,

	$$ ||D-D'||_{\textit{SD}} :=\frac{1}{2}\sum_{d\in \mathcal{X}} |\Pr[D=d] - \Pr[D'=d]| \leq \zeta.$$
\end{defn}

We now give a neighboring relation in terms of a finite sample \vg{from privacy book - Dwork Roth cite?}.

\begin{defn} ref(kasivi?)
Samples (multisets) $Z$ and $Z'$ are neighboring if $z_i \neq z_i'$ for exactly one $\i \in [n]$ (i.e. the Hamming distancebetween $Z$ and $Z'$ is 1).
\end{defn}

\begin{defn}
($\eps$-differential privacy). A randomized algorithm $\A$ is $\eps$-differentially private if for all pairs of neighboring databases, continuous $D, D'$ or discrete $Z, Z'$, and for all sets $\S$ of outputs,
	$$\Pr[\A(D)\in \S] \leq \exp(\eps)\Pr[\A(D')\in \S].$$
\end{defn}

\subsection{Exact Fairness}

The first fairness setting we consider is exact fairness. Recall that in this context we have access to a distribution $D$ over data universe $\X$.

\begin{defn}
	 (Equal Opportunity). We say that a binary predictor $h$ satisfies equal opportunity with respect to $A$ and $Y$ if
	$$\Pr\{h = 1 | A =1, Y=1\} = \Pr\{h = 1 | A=0, Y=1\}.$$
\end{defn}

\subsection{Approximate Fairness}
We now turn to the approximate fairness setting with and define analogous definitions from the exact fairness setting. We now have access to a finite sample $Z$ consisting of entries drawn iid. from an arbitrary distribution $D$ over $\X$.

Define $\zya := \{z_i \in Z | y_i = y, a_i = a \}$ and

An analogous definition for the group-conditional true positive rates:
$$\gamma_{ya}^Z(h) = \frac{1}{|\zya|} \sum_{\zya} h(x,a)$$

\begin{defn}
	($\alpha$-discrimination [srebro ref]). We say that a binary predictor $h$ is $\alpha$-discriminatory with respect to a binary protected attribute $A$ on the population or on a sample $Z$ if, respectively.

$$\Gamma(h) := \max_{y\in \{0,1\}}|\gamma_{y0}(h) - \gamma_{y1}(h)| \leq \alpha ~~ or $$
$$\Gamma^Z(h) := \max_{y\in \{0,1\}}|\gamma_{y0}^Z(h) - \gamma_{y1}^Z(h)| \leq \alpha$$

\end{defn}
When $\alpha = 0$ we are in the exact fairness setting. Note that by [Srebro ref] the sample fairness measure $\Gamma^Z$ converges to the population measure $\Gamma$ when $n$ is large. To achieve an approximate analog to equality of opportunity in definition 2. We define:

$$\Gamma^Z(h) := |\gamma_{10}^Z(h) - \gamma_{11}^Z(h)| \leq \alpha.$$

We now focus on the equality of opportunity setting.
\subsection{Preliminaries from learning theory}
\dk {finish writing}
Formulate finite sample setting with arbitrary distribution D over X,Y. State goal of the learner. State first traditional error minimization. Talk about reformulation into also considering fairness. \dk {do we need to factor a weight $\lambda$ into the new error function? ie. $\ell + \lambda \Gamma$}

\begin{defn}
	(Agnostic PAC Learning) A concept class $\C$ over $\X$ is PAC learnable using hypothesis class $\H$ if there exists an algorithm $\A$ and a polynomial $poly(\cdot,\cdot)$ such that for all $d \in \mathbb{N}$, all concepts $c \in \mathcal{C}_d$, all distributions $D$ on $\X$ and all $\alpha,\beta \in (0,1/2)$, given inputs $\alpha,\beta,$ and $Z = (z_1, ... z_n)$, where n = $poly(d, 1/\alpha, \log(1/\beta))$, $z_i = (x_i, a_i, y_i)$ drawn i.i.d. from D for $i \in [n]$, algorithm $\A$ outpus a hypothesis $h \in \H$ satisfying
	$$\Pr[err(h) \leq OPT + \alpha] \geq 1-\beta.$$

\end{defn}
\begin{defn}
	(Approximately Fair and Private Agnostic PAC Learning)
	Let $d, \alpha, \beta$ be as in Definition 1.7 and $\eps > 0$. Concept class $\C$ is (inefficiently) approximately fair and private agnostically learnable using hypothesis class $\H$ if there exists an algorithm $\A$ that takes inputs $\eps, \alpha, \beta, Z$, where $|Z|=n$ is polynomial in $1/\eps, d, 1\alpha, \log(1/\beta)$ and satisfies \dk {edit fairness criterion}
	\begin{enumerate}
		\item {[}Fairness{]} Algorithm $\A$ satisfies $\Pr[\Gamma(h) \leq OPT + \alpha] \geq 1-\beta$;
		\item {[}Privacy{]} For all $\eps>0$, algorithm $\A(\eps, \cdot, \cdot, \cdot)$ is $\eps$-differentially private;
		\item {[}Utility{]} Algorithm $\A$ agnostically PAC learns $\C$ using $H$.
	\end{enumerate}
\end{defn}

\section{Achieving exact fairness and differential privacy is impossible}
We start with the the continuous setting (which can be translated to the discrete sample setting later). Let $\mathcal{X}$ be the data universe with elements $d_i = (y_i,x_i,a_i) \in \mathcal{X}$ where each entry consists respectively of the response, features, and protected attribute. Note that $x_i$ may have arbitrary correlation with $a_i$, including containing a copy of it.

\dk {... needs more work/ I'm not sure what to put}
In this setting, we have full access to the continuous data distribution $D$ over the data universe $\X$ containing labeled examples $d_i = (y_i,x_i,a_i)$. Our goal is to output a hypothesis that minimizes error while adhering to some notion of fairness and is differentially private. The $Y_i$ are related to $X_i$ by some concept drawn from a concept class $\C$




\begin{defn}
	For ease of notation we define
	$$D_{ya} := \{d_i \in D | y_i = y, a_i = a \}.$$
\end{defn}
\begin{defn}
	(non-trivial hypothesis class). We say that a hypothesis class $\H$ is {\bf non-trivial} if there exists $d_1, d_0 \in \mathcal{X}$ such that for some $h \in \H$, $h(x_1) > h(x_0)$.
\end{defn}
\begin{lemma}Let $\mathcal{H}$ be a non-trivial hypothesis class. Releasing an exactly fair hypothesis $h\in \mathcal{H}$ in a differentially private manner is impossible.
\end{lemma}
\dk{ (currently in weaker eq. of opp. fairness notion, can also be done for stronger notions.) TODO: general proof of impossibility given a `reasonable' fairness constraint on the confusion matrix? Seems like it would be difficult since need to construct neighboring databases. Hence TODO: proof for eq. odds? or say easy extension into other `reasonable' fairness notions.}

\begin{proof}
	Let $D, D'$ be $\zt$-close and suppose that the fair (as in definition 2) hypotheses $h$ is released by algorithm $\mathcal{A}$ on input of $D$. Let $d(\cdot)$ be the measure as defined by the distribution $D$. Pick $d_1, d_0 \in D_{11}$ such that $h(x_1) > h(x_0)$ and assume that $d_1, d_0 \in supp(D) \text{ and } supp(D')$. Let $\zt < \min_{i,D,D'}(d_{D}(d_i))$ $D'$ be a database with $\zt$ more probability mass of $d_1$ and $\zt$ less mass of $d_0$. Then recalling that $d_i = (y_i, x_i, a_i)$:

$$\Pr_{D'}\{h(x) = 1 | a = 1, y=1\} = \int_{D_{11}}h(x)dx + \frac{\zt}{d(D_{11})} h(x_1) - \frac{\zt}{d(D_{11})} h(x_0) $$
$$= \Pr_{D}\{h(x) = 1 | a = 1, y=1\} + \frac{\zt}{d(D_{11})} (h(x_1) - h(x_0))$$
$$>  \Pr_{D}\{h(x) = 1 | a = 1, y=1\}$$

where the last inequality is due to the assumption that $h(x_1) > h(x_0)$. Hence hypothesis $h$ is unfair when applied to database $D'$ and cannot be released by the exactly fair mechanism $\A$ on input of database $D'$. So

$$\Pr\{\mathcal{A}(D) = h\} \not\leq \exp(\eps)\Pr\{\mathcal{A}(D') = h\} = 0$$.

Which violates the definition of privacy in definition 1.
\end{proof}

{\bf Lemma 2} $(\eps, \zt)$-DP and fairness is impossible. Need proof




{\bf Corollary 1} $\Gamma(h) = |\Pr\{h(x) = 1 | y=1, a =0\} - \Pr\{h(x) = 1 | y = 1, a = 1\}|$ has sensitivity $\max(\frac{1}{|\z10|},\frac{1}{|Z_{11}|}, \frac{\gamma_{10}}{|\z10|-1} + \frac{\gamma_{11}}{|Z_{11}|+1}, \frac{\gamma_{10}}{|\z10|+1} + \frac{\gamma_{11}}{|Z_{11}|-1})$

{\bf Proof 1}
We examine two cases:

Case 1: neighboring database $Z'$ is a change in entry within a subgroup ie. within $Z_{1a}$. WLOG let $\alpha = 0$. Let $z \in \z10$ be replaced with $z' \in Z'_{10}$. Then $\gamma^{Z'}_{11} = \gamma^Z_{11}$ but

$$\gamma_{10}^Z(h) = \frac{1}{|\z10|} \sum_{\z10} h(x,0)$$.
$$\gamma_{10}^{Z'}(h) = \frac{1}{|\z10|} (h(x', 0) +\sum_{z \in \z10 \cup Z'_{10}} h(x, 0))$$.

Then,

$$ \Gamma^{Z'}(h) = \gamma_{10}^{Z'}(h) - \gamma_{10}^{Z'}(h) $$
$$ = \gamma_{10}^{Z'}(h) - \gamma^Z_{11}(h) $$
$$ \leq \frac{1}{|\z10|}  + \Gamma^{Z}(h) $$


Case 2: neighboring database is not a change in entry within a subgroup. Thus WLOG $Z' =(Z_{11}\setminus \{z_1\} )\cup( \z10\setminus \{z_0\})$. Let $\gamma_{11}^{Z}(h) <\gamma_{10}^{Z}(h)$ then

$$\gamma_{11}^{Z'}(h) = \frac{1}{|Z'_{1a}|} \sum_{Z'_{1a}} h(x)$$
$$= \frac{1}{|Z'_{11}|} (\sum_{Z_{11}} h(x)-h(x_1)) = \frac{1}{|Z_{11}|-1} (\sum_{Z_{11}} h(x)-h(x_1))$$
Similarly,
$$\gamma_{10}^{Z'}(h) = \frac{1}{|Z'_{10}|} \sum_{Z'_{10}} h(x)$$
$$ = \frac{1}{|Z'_{10}|} (\sum_{\z10} h(x)+h(z_0))= \frac{1}{|\z10|+1} (\sum_{\z10} h(x)+h(z_0))$$

Hence with notation $ \Gamma^Z = \Gamma^Z(h)$ for clarity (\dk{see appendix for details on include here? Refactor and add maxes}), $$|\Gamma^{Z'}- \Gamma^{Z}| = (\gamma_{10}^{Z'} - \gamma_{11}^{Z'}) - (\gamma_{10}^{Z} - \gamma_{11}^{Z})$$


$$=(\gamma_{10}^{Z'}- \gamma_{10}^{Z}) + (\gamma_{11}^{Z} -\gamma_{11}^{Z'})$$
Let $\zt_{10}= \gamma_{10}^{Z'}- \gamma_{10}^{Z}$ and  $\zt_{11} = \gamma_{11}^{Z} -\gamma_{11}^{Z'}$

$$\zt_{10} = \frac{1}{|\z10|+1} (\sum_{\z10} h(x)+h(z_0)) - \gamma_{10}^{Z}$$
$$\zt_{11} = \gamma_{11}^{Z}- \frac{1}{|Z_{11}|-1} (\sum_{Z_{11}} h(x)-h(x_1))$$
\dk {insert template}

$$|\Gamma^{Z'}- \Gamma^{Z}| = \zt_{10} + \zt_{11}= \frac{\gamma_{10}}{|\z10|-1} + \frac{\gamma_{11}}{|Z_{11}|+1} $$

{\bf Proof 2}

Construct $\zt$-close neighboring databases $D$ and $D'$ such that

$$\Pr[D'=d_{10}] - \Pr[D=d_{10}] = \zt$$
$$\Pr[D=d_{11}] - \Pr[D'=d_{11}] = \zt$$



and assume that $d_0, d_1 \in \text{supp}(D')$ and $\in \text{supp}(D)$. Let $d_{10} \in D_{10}$ and $d_{11} \in D_{11}$ and WLOG $\gamma_{10}^Z(h) > \gamma_{11}^Z(h)$ then,

$$\Gamma^{D'}(h) = \gamma_{10}^{D'}(h) - \gamma_{11}^{D'}(h)$$
$$= \int_{D'_{10}}h(x)dx - \int_{D'_{11}}h(x)dx$$
$$= \int_{D_{10}}h(x)dx + \frac{\zt}{dD_{10}}h(x_{10}) - \int_{D'_{11}}h(x)dx - \frac{\zt}{dD_{11}}h(x_{11})$$
$$\leq \Gamma^{D}(h) + \frac{\zt}{dD_{10}} + \frac{\zt}{dD_{11}}$$

where $dD_{1a}$ is the measure of the set $D_{1a}, a \in \{0,1\}$. Hence
$$\zt\Gamma = \frac{\zt}{dD_{10}} + \frac{\zt}{dD_{11}}$$

\dk{Q: more elegant/complete proof? right now it seems we are doing by cases. ie. $\zt$ close changes $\gamma_{ya}(h)$ by at most ... therefore ..
Q: should we do this proof in finite sample context or just translate this to the finite sample (with $\zt = 1/n$)}

\section{Approximate fairness with differentially privacy}



We now turn to the finite sample setting where we release a hypothesis that minimizes the training error. Our goal is now approximate fairness. We use the exponential mechanism in the same way as it is used in private PAC learning. Defining the sample as $Z$, $|Z| = n$ and $\Gamma^{Z}$ as our in-sample fairness measure, we give the algorithm:
$$\mathcal{A}^\eps : \text{Output hypothesis }h \in \mathcal{H} \text{ with probability proportional to }$$
\begin{align}
\exp(-\frac{\eps \cdot u(Z,h)}{2\zt u})
\end{align}

where

$$u(Z,h) = ||(\Gamma^Z(h), \ell^Z(h)||_{1}$$.
$$\zt u(Z,h) = ||(\zt\ell,\zt{\Gamma})||_1 \approx O(||1/n,1/|\z10|+1/|Z_{11}|||_1)$$

$$\ell^Z(h) = \frac{1}{n} \sum_{(x,y) \in Z}\Pr[h(x) \neq y]$$



This algorithm is the exponential mechanism in McSherry and Talwar, and so it is differentially private.

{\bf Lemma 2} The algorithm $\A_\eps$ is $\eps$-differentially private.
Need proof?

Note that except for when $|\H|$ is polynomial, the exponential mechanism does not necessarily yield a polynomial time algorithm.

\dk{refactor constants}
{\bf Theorem 1}(Generic private fair learner) For all $d \in \mathbb{N}$, any concept class $\mathcal{C}_d$ whose cardinality is at most $\exp(\text{poly}(d))$ is privately fairly agnostically learnable using $\H_d = \C_d$. More precisely, the learner uses $n = ..$ labeled examples from $D$, where $\eps, \alpha$, and $\beta$ are parameters of the private learning.
%\dk{Q: ``the domain and range of the concepts in $\mathcal{C}$ is understood to be ensembles $X = \{X_d\}_{d\in \mathbb{N}}$ and $Y = \{Y_d\}_{d\in \mathbb{N}}$, where the representation of elements $X_d, Y_d$ is of size at most $d$''}

{\it Proof.} Let $\A_{\eps}$ be as defined above. The privacy condition is satisfied by Lemma.

Now we show that the utility condition is also satisfied. Let the event $E = \{\A_{\eps} = h \text{ with } u(h) > OPT + \alpha\}$. We need that $\Pr[E] \leq \beta$. We define the utility of $h$ as

$$u(Z,h) = ||(\Gamma^Z(h), \ell^Z(h)||_{1}$$.

By Chernoff-Hoeffding bounds (insert appendix ref. see below proof for now),



$$\Pr[|u(Z,h) - u(D,h)| \geq \rho] \leq 4\exp(\frac{-\rho^2n}{2})$$

for all hypotheses $h \in \H_d$. Hence,

$$\Pr[|u(Z,h) - u(D,h)| \geq \rho \text{ for some } h \in \H_d] \leq 4|\H_d|\exp(\frac{-\rho^2n}{2})$$

\dk{need following proof? relatively similar to kasiviswanathan what can we learn privately}
Now we analyze $\A_\eps(Z)$ conditioned on the event that for all $h\in \H_d$, $|u(Z,h) - u(D,h)| < \rho$. For every $h \in \H_d$, $\Pr[\A_\eps(Z) = h]$ is

$$\frac{\exp(-\frac{\eps}{2\zt u} \cdot u(Z,h))}{\sum_{h'\in\H_d}\exp(-\frac{\eps}{2\zt u} \cdot u(Z,h'))} \leq \frac{\exp(-\frac{\eps}{2\zt u} \cdot u(Z,h))}{\max_{h'\in\H_d}\exp(-\frac{\eps}{2\zt u} \cdot u(Z,h'))} $$
$$= \exp(-\frac{\eps}{2\zt u}(u(Z,h) - \min_{h'\in\H_d}u(Z,h')))$$
$$\leq \exp(-\frac{\eps}{2\zt u}(u(Z,h) - (OPT + \rho)))$$

Hence the probability that $\A_\eps(Z)$ outputs a hypothesis $h \in \H_d$ such that $u(Z,h) > OPT + 2\rho$ is at most $|\H_d|\exp(-\frac{\eps\cdot\rho}{2\zt u})$

Setting $\rho = \alpha/3$. If $u(D,h) \geq OPT + \alpha$ then $|u(D,h) - u(Z,h)| \geq \alpha/3$ or $u(Z,h) \geq OPT + 2\alpha/3$. Hence

$$\Pr[E] \leq |\H_d|(4\exp(\frac{-\alpha^2n}{18}) + \exp(-\frac{\eps\cdot\alpha}{6\zt u})) \leq \beta$$.

Where the inequality holds for $n \geq $. $\square$

{\bf Theorem} (Real-valued Additive Chernoff-Hoeffding Bound). Let $X_1,...,X_d$ be i.i.d. random variables with $\mathbb{E}[X_i] = \mu$ and $a \leq X_i \leq b$ for all $i$. Then for every $\rho > 0$,

$$Pr[|\frac{\sum_i X_i}{n} - \mu| > \rho] \leq 2\exp(\frac{-2\rho^2n}{(b-a)^2})$$

\dk {needs checking, refactor constants}
Chernoff bounds for $u(Z,h)$:

Let $$X_i^a = n(\frac{\1_{z\in Z_{1a}}h(x)}{|Z_{1a}|} - \frac{\1_{z\in Z_{1\neg a}}h(x)}{|Z_{1 \neg a}|} ) + \Pr[h(x) \neq y]$$

Then $$\max_{a\in \{0,1\}}\frac{1}{n} \sum_Z X_i^a =\frac{1}{n}  \max_{a\in \{0,1\}} \sum_Z n(\frac{\1_{z\in Z_{1a}}h(x)}{|Z_{1a}|} - \frac{\1_{z\in Z_{1\neg a}}h(x)}{|Z_{1 \neg a}|} ) + \Pr[h(x) \neq y]$$

$$= |\sum_Z \frac{\1_{z\in \z10}h(x)}{|\z10|} - \frac{\1_{z\in Z_{11}}h(x)}{|Z_{11}|}| +  \sum_Z \frac{\Pr[h(x) \neq y] }{n}$$

$$= |\frac{1}{|\z10|} \sum_{z\in \z10} h(x) - \frac{1}{|Z_{11}|} \sum_{z\in Z_{11}} h(x)| +  \sum_Z \frac{\Pr[h(x) \neq y] }{n}$$

$$=|\gamma_{10}^Z - \gamma_{11}^Z| + \ell^Z(h) $$
$$=||(\Gamma^Z(h), \ell^Z(h)||_{1} = u(Z,h)$$

Hence

$$\Pr[|\frac{1}{n} \sum_Z X_i^a - \mathbb{E}[X_i^a]| > \rho] \leq 2\exp(\frac{-2\rho^2n}{(2-(-1))^2})$$

By union bound (over the choice of $a \in \{0,1\}$

$$\Pr[|u(Z,h) - u(D,h)| > \rho] \leq 4\exp(\frac{-2\rho^2n}{9})$$

\dk {Note: can do extension into other definitions of fairness (with constant factor).
Can also extend into a different norm for $u(Z,h)$ if we have a concentration of measure theorem for the different norm. There are also random matrix concentration bounds (can do concentration for all functions of the confusion matrix?)}

\section{Approximately fair correction with differentially private linear programs}
Do DP-LP

\section{A polynomial time algorithm \\for approximately fair and private classification}
\clearpage

Old stuff:

Our initial approach was to look at this algorithm that achieves equality of opportunity or equal odds by flipping (with some probability) the label of the original non-fair algorithm, $\hat{Y}$, depending on the output label and the class membership $A$. $\hat{Y}$ is trained on $(X,Y)$, the features $X$ and the true label $Y$. When we flip labels given by $\hat{Y}$ we assume that we are given full access to the distribution $(Y,A,\hat{Y})$.

Essentially, modifying $\hat{Y}$ we create a new classifier $\tilde{Y}$ that is fair by setting the probabilities $\text{Pr}\{\tilde{Y} = 1 | \hat{Y} = \hat{y}, A = a \}$ for $\hat{y}, a \in \{0,1\}$ appropriately. The ``pipeline'' with the available information at each stage laid out plainly is as follows:

$$(X,Y) \xrightarrow[]{\text{vanilla training}} (\hat{Y},A,Y) \xrightarrow[]{\text{fairness}} \tilde{Y} $$


\section{Privatizing Woodworth et. al. (2017)}
(This is the follow-up to Eq. of Opp.)

\subsection{Brief summary}
This paper gives a two step algorithm in a learning model with finite samples and hypothesis classes. They split the training set into two parts. The first step is traditional ERM (with 0-1 loss) but with a sample based fairness constraint using half of the training set. The second step takes the outputted hypothesis from step 1 and does the same post-processing step as in the algorithm of Equality of Opportunity.


\section{Privatizing first}
With the finite sample setting and also the case that we cannot achieve exact fairness privately, this line of inquiry now seems to be fruitful. It would not take long to analyze the effect of first privatizing via synthetic data or adding noise to the training data (smallDB etc.) and then training a fair classifier.

\section{Appendix}
p-norm discussion from exp mech?



\end{document}
