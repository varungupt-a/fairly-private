\documentclass[format = sigconf]{acmart}
\usepackage{amsmath,amsthm,amssymb,amsfonts}
\usepackage{mathtools}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{fancyhdr}
\usepackage{hyperref}
\usepackage{natbib}
\usepackage{bbm}
\usepackage{enumitem}
\newcommand{\dk}[1]{\textcolor{red}{[DK: #1]}}
\newcommand{\vg}[1]{\textcolor{blue}{[VG: #1]}}


\newcommand\tab[1][0.5cm]{\hspace*{#1}}
\newcommand{\I}{\mathcal{I}}
\newcommand{\M}{\mathcal{M}}
\newcommand{\A}{\mathcal{A}}
\renewcommand{\H}{\mathcal{H}}
\newcommand{\C}{\mathcal{C}}
\newcommand{\X}{\mathcal{X}}
\renewcommand{\S}{\mathcal{S}}


\newcommand{\1}{\mathbbm{1}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}


\newcommand{\eps}{\epsilon}
\newcommand{\zt}{\zeta}
\newcommand{\zya}{Z_{ya}}

\newcommand{\g}[1]{\gamma_{#1}(h)}
\newcommand{\gz}[1]{\gamma_{#1}^Z(h)}

\newcommand{\z}[1]{Z_{#1}}
\renewcommand{\d}[1]{D_{#1}}
\iffalse
\newcommand{\g10}[1]{\gamma_{10}^Z(h)}
\newcommand{\g11}{\gamma_{11}^Z(h)}

\newcommand{\g10'}{\gamma_{10}^{Z'}(h)}
\newcommand{\g11'}{\gamma_{11}^{Z'}(h)}

\newcommand{\GZ}{\Gamma^{Z'}(h)}
\newcommand{\GZ'}{\Gamma^{Z'}(h)}
\fi
\newcommand{\lzh}{\ell^{Z}(h)}


%
\theoremstyle{definition}
\newtheorem{defn}{Definition}[section]

%
\newenvironment{claim}[1]{\par\noindent\underline{Claim:}\space#1}{}
\newenvironment{claimproof}[1]{\par\noindent\underline{Proof:}\space#1}

\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,
    urlcolor=blue,
}

\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}
\DeclarePairedDelimiter{\floor}{\lfloor}{\rfloor}



\newenvironment{problem}[2][Problem]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
%If you want to title your bold things something different just make another thing exactly like this but replace "problem" with the name of the thing you want, like theorem or lemma or whatever

\begin{document}

%\renewcommand{\qedsymbol}{\filledbox}
%Good resources for looking up how to do stuff:
%Binary operators: http://www.access2science.com/latex/Binary.html
%General help: http://en.wikibooks.org/wiki/LaTeX/Mathematics
%Or just google stuff

\title{Privacy $\cap$ Fairness}
{\author{Rachel Cummings, Varun Gupta, Dhamma Kimpara, Jamie Morgenstern}}
\maketitle


\section{Introduction}
\subsection{Related Work}
\subsection{Our Contributions}
We present two contributions to the study of the intersection of differential privacy and fairness in the binary classification setting. First, we show that it is impossible to achieve both differential privacy and exact fairness even in the setting where we have access to the full distribution of the data. We then consider a notion of approximate fairness in the finite sample access setting and show that there exists a private PAC learner that is differentially private and satisfies approximate fairness with high probability. \vg{Finally, we present a polynomial time algorithm to achieve the differentially private and approximately fair scheme outlined above and show its empirical performance on certain datasets.}

\section{Preliminaries}
\dk {Let $\X$ be our data universe consisting of elements $z=(x,a,y)$ where $x$ is the features, $a$ the protected (binary) attribute, and $y$ the binary response. Note that $x$ may have arbitrary correlation with $a$, including containing a copy of it. $\A(x)$ is the probability distribution over outputs of a randomized algorithm $A$ on input $x$.}

% First we consider the interaction of differential privacy and fairness in the full distribution access setting in which we show the impossibility of achieving both.

% We consider data universe $\X$, where we have access to a joint probability distribution $D = (X, A, Y)$ over $\X$, where $X$ are the features, $A$ sensitive attributes, and $Y$ true labels, respectively. Distribution $D$ will represent a database in $\X$, as we specify below. Let $d_i = (x_i, a_i, y_i)$ be instances sampled from the distribution $D$. Note that $x_i$ may have arbitrary correlation with $a_i$, including containing a copy of it.
%  \\ \\
% In the discrete setting:

% We consider data universe $\X$. We have a finite sample database $Z$ with $n$ entries, with labeled examples $z_i = (x_i, a_i, y_i)$, drawn from an abitrary distribution over $\X$. In this context, we define $\zya = \{z \in Z | y_i =y, a_i = a\}$.
\subsection{Differential Privacy}
For our results, we use two notions of a database: 
\begin{enumerate}
	\item a distribution $D$ over $\X$;
	\item a finite sample, vector $Z = (z_1, ..., z_n)$ drawn i.i.d. from a distribution $D$ over $\X$.
\end{enumerate}
For each notion, there is an accompanying notion of a neighboring database:
\begin{defn}
	(1) ($\zt$-closeness). Random variables $D$ and $D'$ taking values in $\X$ are $\zt$-close if the statistical distance between their distributions is at most $\zt$, i.e.,

	$$ ||D-D'||_{\textit{SD}} :=\frac{1}{2}\sum_{d\in \mathcal{X}} |\Pr[D=d] - \Pr[D'=d]| \leq \zeta.$$
\end{defn}

\begin{defn} ref(kasivi?) \vg{from privacy book - Dwork Roth cite?}.
(2) Samples $Z$ and $Z'$ are neighboring if $z_i \neq z_i'$ for exactly one $i \in [n]$ (i.e. the Hamming distance between $Z$ and $Z'$ is 1).
\end{defn}
Alternatively we could define a finite sample as a multi-set, and use the symmetric distance instead of the Hamming metric to measure distance. Note that definition 2.1 is a generalization of definition 2.2. 

For each of these notions, the definition of differential privacy remains the same. That is, a randomized algorithm is private if neighboring databases induce close distributions on its outcomes:

\begin{defn}
($\eps$-differential privacy). A randomized algorithm $\A$ is $\eps$-differentially private if for all pairs of neighboring databases $D,D'$ and for all sets $\S$ of outputs,
	$$\Pr[\A(D)\in \S] \leq \exp(\eps)\Pr[\A(D')\in \S].$$
\end{defn}
\dk{need to talk about composition/post-processing theorems and/or exponenetial mechanism?}
\subsection{Exact Fairness}

In this setting our database notion is (1): a distribution $D$ over $X$. This corresponds with the full distributional access setting of [cite eq of opp hardt] where we have access to the joint probability distribution $D = (X, A, Y)$ over $\X$, where $X$ are the features, $A$ sensitive attributes, and $Y$ true labels, respectively. Hence we state our notion of exact fairness in these terms:

\begin{defn}
	 (Equal Opportunity). We say that a binary predictor $h$ satisfies equal opportunity with respect to $A$ and $Y$ if
	$$\Pr\{h = 1 | A =1, Y=1\} = \Pr\{h = 1 | A=0, Y=1\}.$$
\end{defn}

Note that this is the weaker of the two definitions of fairness in Hardt et. al. however, it is sufficient to prove impossibility for this notion since violating this notion implies violating the stronger notion, equal odds. These measures of fairness belong to a broader class of fairness constraints that is, constraints on functions of the confusion matrix of a classifier (convex constraints paper). 

\subsection{Approximate Fairness}
We now turn to the approximate fairness setting with and define analogous definitions from the exact fairness setting. Here our notion of a database is (2) i.e. we have access to a finite sample $Z$ consisting of entries drawn i.i.d. from an arbitrary distribution $D$ over $\X$. 

For ease of notation we define $\zya := \{z_i \in Z | y_i = y, a_i = a \}$.

We also have an analogous definition for the group-conditional true positive rates: \dk {clarify summation as such?: $\sum_{z_i \in \zya} h(x_i)$}
$$\gamma_{ya}^Z(h) = \frac{1}{|\zya|} \sum_{\zya} h(x).$$

\begin{defn}
	($\alpha$-discrimination [srebro ref]). We say that a binary predictor $h$ is $\alpha$-discriminatory with respect to a binary protected attribute $A$ on the population or on a sample $Z$ if, respectively.

$$\Gamma(h) := \max_{y\in \{0,1\}}|\g{y0} - \g{y1}| \leq \alpha ~~ or $$
$$\Gamma^Z(h) := \max_{y\in \{0,1\}}|\gz{y0} - \gz{y1}| \leq \alpha$$

\end{defn}
When $\alpha = 0$ we are in the exact fairness setting. Note that by [Srebro ref] the sample fairness measure $\Gamma^Z$ converges to the population measure $\Gamma$ when $n$ is large. To achieve an approximate analog to equality of opportunity in definition 2. For the analog of Definition 2.4 (eq. opp) where we only consider true positive rates, the definition reduces to:

$$\Gamma^Z(h) := |\gz{10} - \gz{11}| \leq \alpha.$$

In the remainder of this paper, $\Gamma$ refers to this fairness measure. \dk {actually we could do an impossibility result for any hard threshold for $\Gamma$ in any setting. Hence motivating approx fairness with high probability}

\dk {talk about motivation for approx fairness because of finite sample (cannot achieve exact) and now also because of impossibility with privacy.}

\subsection{Preliminaries from learning theory}
\dk {do we need to factor a weight $\lambda$ into the new error function? ie. $\ell + \lambda \Gamma$}
\dk {very not sure how to write this section, help?}
A concept is a function that labels examples taken from the domain $X$ by the elements of the range $Y$. A \emph{concept class} is a set of concepts. The domain and range in $\C$ are ensembles $X= \{X_d\}_{d\in \N}$, $Y= \{Y_d\}_{d\in \N}$ where the representation size of the elements in $X_d,Y_d$ is at most $d$. Since we focus on binary classification problems, $d$ measures the size of examples in $X_d$. When the size parameter is clear from the context or not important, we omit the subscript $d$.

In our particular setting, our distribution of labeled examples is arbitrary on $X_d \times Y_d$ and hence we consider the agnostic setting that removes the realizability assumption. The goal of a learner then is to output a hypothesis $h \in \H$ whose error with respect to the distribution is close to the optimal possible by a function from $\C$. The misclassification error of $h$ on $D$ is defined as:

$$err(h) = \Pr_{(x,y) \tilde D}[h(x) \neq y].$$

In our setting we consider the agnostic learnability setting. 
\dk {not sure if following def is correct (wrt. concept class $\C_d$ do we consider $\C$ in the agnostic setting?)}
\begin{defn}
	(Agnostic PAC Learning) A concept class $\C$ over $\X$ is PAC learnable using hypothesis class $\H$ if there exists an algorithm $\A$ and a polynomial $poly(\cdot,\cdot)$ such that for all $d \in \mathbb{N}$, all concepts $c \in \mathcal{C}_d$, all distributions $D$ on $\X$ and all $\alpha,\beta \in (0,1/2)$, given inputs $\alpha,\beta,$ and $Z = (z_1, ... z_n)$, where n = $poly(d, 1/\alpha, \log(1/\beta))$, $z_i = (x_i, a_i, y_i)$ drawn i.i.d. from D for $i \in [n]$, algorithm $\A$ outpus a hypothesis $h \in \H$ satisfying
	$$\Pr[err(h) \leq OPT + \alpha] \geq 1-\beta.$$

\end{defn}
\dk {insert note about distribution-free learning}
\subsection{Private and Approximately Fair Agnostic Learning}
We define private and approximately fair PAC learners as algorithms that satisfy the definitions of differential privacy, approximate fairness (with high probability), and PAC learning.
\begin{defn}
	(Private and Approximately Fair Agnostic PAC Learning)
	Let $d, \alpha, \beta$ be as in Definition 2.6 (agnostic PAC learnigng) and $\eps > 0$. Concept class $\C$ is (inefficiently) privately and approximately fair agnostically learnable using hypothesis class $\H$ if there exists an algorithm $\A$ that takes inputs $\eps, \alpha, \beta, Z$, where $|Z|=n$ is polynomial in $1/\eps, d, 1\alpha, \log(1/\beta)$ and satisfies 
	\begin{enumerate}
		\item {[}Fairness{]} Algorithm $\A$ satisfies $\Pr[\Gamma(h) \leq OPT + \alpha] \geq 1-\beta$;
		\item {[}Privacy{]} For all $\eps>0$, algorithm $\A(\eps, \cdot, \cdot, \cdot)$ is $\eps$-differentially private;
		\item {[}Utility{]} Algorithm $\A$ agnostically PAC learns $\C$ using $H$.
	\end{enumerate}
\end{defn}

\section{Achieving exact fairness and differential privacy is impossible}
We start with the the continuous setting (which can be translated to the discrete sample setting later). Let $\mathcal{X}$ be the data universe with elements $d_i = (y_i,x_i,a_i) \in \mathcal{X}$ where each entry consists respectively of the response, features, and protected attribute. Note that $x_i$ may have arbitrary correlation with $a_i$, including containing a copy of it.

\dk {... needs more work/ I'm not sure what to put}
In this setting, we have full access to the continuous data distribution $D$ over the data universe $\X$ containing labeled examples $d_i = (y_i,x_i,a_i)$. Our goal is to output a hypothesis that minimizes error while adhering to some notion of fairness and is differentially private. The $Y_i$ are related to $X_i$ by some concept drawn from a concept class $\C$




\begin{defn}
	For ease of notation we define
	$$D_{ya} := \{d_i \in D | y_i = y, a_i = a \}.$$
\end{defn}
\begin{defn}
	(non-trivial hypothesis class). We say that a hypothesis class $\H$ is {\bf non-trivial} if there exists $d_1, d_0 \in \mathcal{X}$ such that for some $h \in \H$, $h(x_1) > h(x_0)$.
\end{defn}
\begin{lemma}Let $\mathcal{H}$ be a non-trivial hypothesis class. Releasing an exactly fair hypothesis $h\in \mathcal{H}$ in a differentially private manner is impossible.
\end{lemma}
\dk{ (currently in weaker eq. of opp. fairness notion, can also be done for stronger notions.) TODO: general proof of impossibility given a `reasonable' fairness constraint on the confusion matrix? Seems like it would be difficult since need to construct neighboring databases. Hence TODO: proof for eq. odds? or say easy extension into other `reasonable' fairness notions.}

\begin{proof}
	Let $D, D'$ be $\zt$-close and suppose that the fair (as in definition 2) hypotheses $h$ is released by algorithm $\mathcal{A}$ on input of $D$. Let $d(\cdot)$ be the measure as defined by the distribution $D$. Pick $d_1, d_0 \in D_{11}$ such that $h(x_1) > h(x_0)$ and assume that $d_1, d_0 \in supp(D) \text{ and } supp(D')$. Let $\zt < \min_{i,D,D'}(d_{D}(d_i))$ $D'$ be a database with $\zt$ more probability mass of $d_1$ and $\zt$ less mass of $d_0$. Then recalling that $d_i = (y_i, x_i, a_i)$:

$$\Pr_{D'}\{h(x) = 1 | a = 1, y=1\} = \int_{D_{11}}h(x)dx + \frac{\zt}{d(D_{11})} h(x_1) - \frac{\zt}{d(D_{11})} h(x_0) $$
$$= \Pr_{D}\{h(x) = 1 | a = 1, y=1\} + \frac{\zt}{d(D_{11})} (h(x_1) - h(x_0))$$
$$>  \Pr_{D}\{h(x) = 1 | a = 1, y=1\}$$

where the last inequality is due to the assumption that $h(x_1) > h(x_0)$. Hence hypothesis $h$ is unfair when applied to database $D'$ and cannot be released by the exactly fair mechanism $\A$ on input of database $D'$. So

$$\Pr\{\mathcal{A}(D) = h\} \not\leq \exp(\eps)\Pr\{\mathcal{A}(D') = h\} = 0$$.

Which violates the definition of privacy in definition 1.
\end{proof}

{\bf Lemma 2} $(\eps, \zt)$-DP and fairness is impossible. Need proof




{\bf Corollary 1} $\Gamma(h) = |\Pr\{h(x) = 1 | y=1, a =0\} - \Pr\{h(x) = 1 | y = 1, a = 1\}|$ has sensitivity $\max(\frac{1}{\z{10}},\frac{1}{|Z_{11}|}, \frac{\gamma_{10}}{|\z{10}|-1} + \frac{\gamma_{11}}{|Z_{11}|+1}, \frac{\gamma_{10}}{|\z{10}|+1} + \frac{\gamma_{11}}{|Z_{11}|-1})$

{\bf Proof 1}
We examine two cases:

Case 1: neighboring database $Z'$ is a change in entry within a subgroup ie. within $Z_{1a}$. WLOG let $\alpha = 0$. Let $z \in \z{10}$ be replaced with $z' \in Z'_{10}$. Then $\gamma^{Z'}_{11} = \gamma^Z_{11}$ but

$$\gamma_{10}^Z(h) = \frac{1}{|\z{10}|} \sum_{\z{10}} h(x,0)$$.
$$\gamma_{10}^{Z'}(h) = \frac{1}{|\z{10}|} (h(x', 0) +\sum_{z \in \z{10} \cup Z'_{10}} h(x, 0))$$.

Then,

$$ \Gamma^{Z'}(h) = \gamma_{10}^{Z'}(h) - \gamma_{10}^{Z'}(h) $$
$$ = \gamma_{10}^{Z'}(h) - \gz{11} $$
$$ \leq \frac{1}{|\z{10}|}  + \Gamma^{Z}(h) $$


Case 2: neighboring database is not a change in entry within a subgroup. Thus WLOG $Z' =(Z_{11}\setminus \{z_1\} )\cup( \z{10}\setminus \{z_0\})$. Let $\gamma_{11}^{Z}(h) <\gamma_{10}^{Z}(h)$ then

$$\gamma_{11}^{Z'}(h) = \frac{1}{|Z'_{1a}|} \sum_{Z'_{1a}} h(x)$$
$$= \frac{1}{|Z'_{11}|} (\sum_{Z_{11}} h(x)-h(x_1)) = \frac{1}{|Z_{11}|-1} (\sum_{Z_{11}} h(x)-h(x_1))$$
Similarly,
$$\gamma_{10}^{Z'}(h) = \frac{1}{|Z'_{10}|} \sum_{Z'_{10}} h(x)$$
$$ = \frac{1}{|Z'_{10}|} (\sum_{\z{10}} h(x)+h(z_0))= \frac{1}{|\z{10}|+1} (\sum_{\z{10}} h(x)+h(z_0))$$

Hence with notation $ \Gamma^Z = \Gamma^Z(h)$ for clarity (\dk{see appendix for details on include here? Refactor and add maxes}), $$|\Gamma^{Z'}- \Gamma^{Z}| = (\gamma_{10}^{Z'} - \gamma_{11}^{Z'}) - (\gamma_{10}^{Z} - \gamma_{11}^{Z})$$


$$=(\gamma_{10}^{Z'}- \gamma_{10}^{Z}) + (\gamma_{11}^{Z} -\gamma_{11}^{Z'})$$
Let $\zt_{10}= \gamma_{10}^{Z'}- \gamma_{10}^{Z}$ and  $\zt_{11} = \gamma_{11}^{Z} -\gamma_{11}^{Z'}$

$$\zt_{10} = \frac{1}{|\z{10}|+1} (\sum_{\z{10}} h(x)+h(z_0)) - \gamma_{10}^{Z}$$
$$\zt_{11} = \gamma_{11}^{Z}- \frac{1}{|Z_{11}|-1} (\sum_{Z_{11}} h(x)-h(x_1))$$
\dk {insert template}

$$|\Gamma^{Z'}- \Gamma^{Z}| = \zt_{10} + \zt_{11}= \frac{\gamma_{10}}{|\z{10}|-1} + \frac{\gamma_{11}}{|Z_{11}|+1} $$

{\bf Proof 2}

Construct $\zt$-close neighboring databases $D$ and $D'$ such that

$$\Pr[D'=d_{10}] - \Pr[D=d_{10}] = \zt$$
$$\Pr[D=d_{11}] - \Pr[D'=d_{11}] = \zt$$



and assume that $d_0, d_1 \in \text{supp}(D')$ and $\in \text{supp}(D)$. Let $d_{10} \in D_{10}$ and $d_{11} \in D_{11}$ and WLOG $\gamma_{10}^Z(h) > \gamma_{11}^Z(h)$ then,

$$\Gamma^{D'}(h) = \gamma_{10}^{D'}(h) - \gamma_{11}^{D'}(h)$$
$$= \int_{D'_{10}}h(x)dx - \int_{D'_{11}}h(x)dx$$
$$= \int_{D_{10}}h(x)dx + \frac{\zt}{dD_{10}}h(x_{10}) - \int_{D'_{11}}h(x)dx - \frac{\zt}{dD_{11}}h(x_{11})$$
$$\leq \Gamma^{D}(h) + \frac{\zt}{dD_{10}} + \frac{\zt}{dD_{11}}$$

where $dD_{1a}$ is the measure of the set $D_{1a}, a \in \{0,1\}$. Hence
$$\zt\Gamma = \frac{\zt}{dD_{10}} + \frac{\zt}{dD_{11}}$$

\dk{Q: more elegant/complete proof? right now it seems we are doing by cases. ie. $\zt$ close changes $\gamma_{ya}(h)$ by at most ... therefore ..
Q: should we do this proof in finite sample context or just translate this to the finite sample (with $\zt = 1/n$)}

\section{Approximate fairness with differentially privacy}



We now turn to the finite sample setting where we release a hypothesis that minimizes the training error. Our goal is now approximate fairness. We use the exponential mechanism in the same way as it is used in private PAC learning. Defining the sample as $Z$, $|Z| = n$ and $\Gamma^{Z}$ as our in-sample fairness measure, we give the algorithm:
$$\mathcal{A}^\eps : \text{Output hypothesis }h \in \mathcal{H} \text{ with probability proportional to }$$
\begin{align}
\exp(-\frac{\eps \cdot u(Z,h)}{2\zt u})
\end{align}

where

$$u(Z,h) = ||(\Gamma^Z(h), \ell^Z(h)||_{1}$$.
$$\zt u(Z,h) = ||(\zt\ell,\zt{\Gamma})||_1 \approx O(||1/n,1/|\z{10}|+1/|Z_{11}|||_1)$$

$$\ell^Z(h) = \frac{1}{n} \sum_{(x,y) \in Z}\Pr[h(x) \neq y]$$



This algorithm is the exponential mechanism in McSherry and Talwar, and so it is differentially private.

{\bf Lemma 2} The algorithm $\A_\eps$ is $\eps$-differentially private.
Need proof?

Note that except for when $|\H|$ is polynomial, the exponential mechanism does not necessarily yield a polynomial time algorithm.

\dk{refactor constants}
{\bf Theorem 1}(Generic private fair learner) For all $d \in \mathbb{N}$, any concept class $\mathcal{C}_d$ whose cardinality is at most $\exp(\text{poly}(d))$ is privately fairly agnostically learnable using $\H_d = \C_d$. More precisely, the learner uses $n = ..$ labeled examples from $D$, where $\eps, \alpha$, and $\beta$ are parameters of the private learning.
%\dk{Q: ``the domain and range of the concepts in $\mathcal{C}$ is understood to be ensembles $X = \{X_d\}_{d\in \mathbb{N}}$ and $Y = \{Y_d\}_{d\in \mathbb{N}}$, where the representation of elements $X_d, Y_d$ is of size at most $d$''}

{\it Proof.} Let $\A_{\eps}$ be as defined above. The privacy condition is satisfied by Lemma.

Now we show that the utility condition is also satisfied. Let the event $E = \{\A_{\eps} = h \text{ with } u(h) > OPT + \alpha\}$. We need that $\Pr[E] \leq \beta$. We define the utility of $h$ as

$$u(Z,h) = ||(\Gamma^Z(h), \ell^Z(h)||_{1}$$.

By Chernoff-Hoeffding bounds (insert appendix ref. see below proof for now),



$$\Pr[|u(Z,h) - u(D,h)| \geq \rho] \leq 4\exp(\frac{-\rho^2n}{2})$$

for all hypotheses $h \in \H_d$. Hence,

$$\Pr[|u(Z,h) - u(D,h)| \geq \rho \text{ for some } h \in \H_d] \leq 4|\H_d|\exp(\frac{-\rho^2n}{2})$$

\dk{need following proof? relatively similar to kasiviswanathan what can we learn privately}
Now we analyze $\A_\eps(Z)$ conditioned on the event that for all $h\in \H_d$, $|u(Z,h) - u(D,h)| < \rho$. For every $h \in \H_d$, $\Pr[\A_\eps(Z) = h]$ is

$$\frac{\exp(-\frac{\eps}{2\zt u} \cdot u(Z,h))}{\sum_{h'\in\H_d}\exp(-\frac{\eps}{2\zt u} \cdot u(Z,h'))} \leq \frac{\exp(-\frac{\eps}{2\zt u} \cdot u(Z,h))}{\max_{h'\in\H_d}\exp(-\frac{\eps}{2\zt u} \cdot u(Z,h'))} $$
$$= \exp(-\frac{\eps}{2\zt u}(u(Z,h) - \min_{h'\in\H_d}u(Z,h')))$$
$$\leq \exp(-\frac{\eps}{2\zt u}(u(Z,h) - (OPT + \rho)))$$

Hence the probability that $\A_\eps(Z)$ outputs a hypothesis $h \in \H_d$ such that $u(Z,h) > OPT + 2\rho$ is at most $|\H_d|\exp(-\frac{\eps\cdot\rho}{2\zt u})$

Setting $\rho = \alpha/3$. If $u(D,h) \geq OPT + \alpha$ then $|u(D,h) - u(Z,h)| \geq \alpha/3$ or $u(Z,h) \geq OPT + 2\alpha/3$. Hence

$$\Pr[E] \leq |\H_d|(4\exp(\frac{-\alpha^2n}{18}) + \exp(-\frac{\eps\cdot\alpha}{6\zt u})) \leq \beta$$.

Where the inequality holds for $n \geq $. $\square$

{\bf Theorem} (Real-valued Additive Chernoff-Hoeffding Bound). Let $X_1,...,X_d$ be i.i.d. random variables with $\mathbb{E}[X_i] = \mu$ and $a \leq X_i \leq b$ for all $i$. Then for every $\rho > 0$,

$$Pr[|\frac{\sum_i X_i}{n} - \mu| > \rho] \leq 2\exp(\frac{-2\rho^2n}{(b-a)^2})$$

\dk {needs checking, refactor constants}
Chernoff bounds for $u(Z,h)$:

Let $$X_i^a = n(\frac{\1_{z\in Z_{1a}}h(x)}{|Z_{1a}|} - \frac{\1_{z\in Z_{1\neg a}}h(x)}{|Z_{1 \neg a}|} ) + \Pr[h(x) \neq y]$$

Then $$\max_{a\in \{0,1\}}\frac{1}{n} \sum_Z X_i^a =\frac{1}{n}  \max_{a\in \{0,1\}} \sum_Z n(\frac{\1_{z\in Z_{1a}}h(x)}{|Z_{1a}|} - \frac{\1_{z\in Z_{1\neg a}}h(x)}{|Z_{1 \neg a}|} ) + \Pr[h(x) \neq y]$$

$$= |\sum_Z \frac{\1_{z\in \z{10}}h(x)}{|\z{10}|} - \frac{\1_{z\in Z_{11}}h(x)}{|Z_{11}|}| +  \sum_Z \frac{\Pr[h(x) \neq y] }{n}$$

$$= |\frac{1}{|\z{10}|} \sum_{z\in \z{10}} h(x) - \frac{1}{|Z_{11}|} \sum_{z\in Z_{11}} h(x)| +  \sum_Z \frac{\Pr[h(x) \neq y] }{n}$$

$$=|\gamma_{10}^Z - \gamma_{11}^Z| + \ell^Z(h) $$
$$=||(\Gamma^Z(h), \ell^Z(h)||_{1} = u(Z,h)$$

Hence

$$\Pr[|\frac{1}{n} \sum_Z X_i^a - \mathbb{E}[X_i^a]| > \rho] \leq 2\exp(\frac{-2\rho^2n}{(2-(-1))^2})$$

By union bound (over the choice of $a \in \{0,1\}$

$$\Pr[|u(Z,h) - u(D,h)| > \rho] \leq 4\exp(\frac{-2\rho^2n}{9})$$

\dk {Note: can do extension into other definitions of fairness (with constant factor).
Can also extend into a different norm for $u(Z,h)$ if we have a concentration of measure theorem for the different norm. There are also random matrix concentration bounds (can do concentration for all functions of the confusion matrix?)}

\section{Approximately fair correction with differentially private linear programs}
Do DP-LP

\section{A polynomial time algorithm \\for approximately fair and private classification}
\clearpage

Old stuff:

Our initial approach was to look at this algorithm that achieves equality of opportunity or equal odds by flipping (with some probability) the label of the original non-fair algorithm, $\hat{Y}$, depending on the output label and the class membership $A$. $\hat{Y}$ is trained on $(X,Y)$, the features $X$ and the true label $Y$. When we flip labels given by $\hat{Y}$ we assume that we are given full access to the distribution $(Y,A,\hat{Y})$.

Essentially, modifying $\hat{Y}$ we create a new classifier $\tilde{Y}$ that is fair by setting the probabilities $\text{Pr}\{\tilde{Y} = 1 | \hat{Y} = \hat{y}, A = a \}$ for $\hat{y}, a \in \{0,1\}$ appropriately. The ``pipeline'' with the available information at each stage laid out plainly is as follows:

$$(X,Y) \xrightarrow[]{\text{vanilla training}} (\hat{Y},A,Y) \xrightarrow[]{\text{fairness}} \tilde{Y} $$


\section{Privatizing Woodworth et. al. (2017)}
(This is the follow-up to Eq. of Opp.)

\subsection{Brief summary}
This paper gives a two step algorithm in a learning model with finite samples and hypothesis classes. They split the training set into two parts. The first step is traditional ERM (with 0-1 loss) but with a sample based fairness constraint using half of the training set. The second step takes the outputted hypothesis from step 1 and does the same post-processing step as in the algorithm of Equality of Opportunity.


\section{Privatizing first}
With the finite sample setting and also the case that we cannot achieve exact fairness privately, this line of inquiry now seems to be fruitful. It would not take long to analyze the effect of first privatizing via synthetic data or adding noise to the training data (smallDB etc.) and then training a fair classifier.

\section{Appendix}
p-norm discussion from exp mech?



\end{document}

