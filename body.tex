% !TEX root = main.tex

\section{Introduction}
Recent applications of machine learning in human-relevant domains
raise concerns that too much of some individual's information might be
leaked through a model learned on some training data. For example, if
one learns a model based on historical health data, many algorithms
have some real possibility of outputting a model containing
information about individuals' HIV status or other sensitive
information.  Resultingly, academia and industry have spent much
effort designing and implementing \emph{differentially private}
machine learning methods.

Differential privacy gives a strong guarantee to individuals whose
data we use to train a model; these human-centric uses of ML systems
have also raised concerns of equity of the predictive power for a
model on different populations. The \emph{fairness} of a model can be
thought of as some equitable performance guarantee for individuals who
will be evaluated by the model, rather than a guarantee for someone
participating in the model's training process. When phrasing privacy
and fairness of a model this way, a natural question arises: in what
settings can we learn a model which is private in the training data
while guaranteeing equitable performance for multiple populations on
which the model will ultimately run? More formally, will it be
possible to guarantee privacy of training data, fairness for the
predictions made on two populations, and some accuracy overall?

\jm{More stuff here}

\subsection{Related Work}

The focus on fairness in machine learning and its relationship to
differential privacy was explored in early work in the community (cite
fairness through awareness). This work introduced the concept of
treating similar people similarly, where ``similarity'' is defined as
some task-specific metric over individuals.  The authors then point
out that this desiderata can be formulated as a Lipschitz constraint
and show how to satisfy it using tools from DP.

More recently, (citet the paper from fat) raised questions of whether
statistical notions of equitable predictive power, such as equalized
odds (cite moritz), are compatible with privacy. In a limiting sense,
when one talks about feature and model selection, there appears to be
some tension here: an additional feature might increase the possible
privacy loss an individual faces, while the additional feature should
only make EO easier to satisfy (as alluded to in Hanna's paper on
exploration and exploitation). However, to the best of the authors'
knowledge, no work has heretofore shown whether an ML algorithm can
guarantee individuals differential privacy and their populations some
kind of group fairness simultaneously.

The technical tools we use for this work come from differential price
vacy, using the exponential mechanism (cite), private SGD for our
empirical results, and FILL IN ANY OTHER TOOLS WE USE.



Related work we definitely want to cover: fairness through awareness,
the paper proposing we study privacy + fairness, dwork's ``it isn't
private and it isn't fair'' or whatever...

\subsection{Our Contributions}
We present two contributions to the study of the intersection of
differential privacy and fairness in the binary classification setting.
First, we show that it is impossible to achieve both differential
privacy and exact fairness even in the setting where we have access to
the full distribution of the data. We then consider a notion of
approximate fairness in the finite sample access setting and show that
there exists a private PAC learner that is differentially private and
satisfies approximate fairness with high probability. Finally, we present some incompatibility results concerning current practical  differentially private and fairness-ensuring learning algorithms (in-process methods). 


\section{Preliminaries}
Let $\X$ be our data universe consisting of elements $z=(x,a,y)$
 where $x$ is the features, $a$ the protected (binary) attribute, and
 $y$ the binary response. Note that $x$ may have arbitrary correlation
 with $a$, including containing a copy of it. $\A(x)$ is the
 probability distribution over outputs of a randomized algorithm $\A$ on
 input $x$.

% First we consider the interaction of differential privacy and fairness in the full distribution access setting in which we show the impossibility of achieving both.

% We consider data universe $\X$, where we have access to a joint probability distribution $D = (X, A, Y)$ over $\X$, where $X$ are the features, $A$ sensitive attributes, and $Y$ true labels, respectively. Distribution $D$ will represent a database in $\X$, as we specify below. Let $d_i = (x_i, a_i, y_i)$ be instances sampled from the distribution $D$. Note that $x_i$ may have arbitrary correlation with $a_i$, including containing a copy of it.
%  \\ \\
% In the discrete setting:

% We consider data universe $\X$. We have a finite sample database $Z$ with $n$ entries, with labeled examples $z_i = (x_i, a_i, y_i)$, drawn from an abitrary distribution over $\X$. In this context, we define $\zya = \{z \in Z | y_i =y, a_i = a\}$.


\subsection{Differential Privacy}

For our results, we use two notions of a database:

\begin{enumerate}
	\item a distribution $D$ over $\X$;
	\item a finite sample, vector $Z = (z_1, ..., z_n)$ drawn i.i.d. from a distribution $D$ over $\X$.
\end{enumerate}

For each notion, there is an accompanying notion of a neighboring
database. We present them here in respective order.

For notion (1), we adapt the definition of neighboring databases in the continuous case from \citep{2dplimits}:

\begin{defn}
  ($\zt$-closeness). Distributions $D$ and $D'$ taking values in
  $\X$ are $\zt$-close if the statistical distance between their
  distributions is at most $\zt$, i.e.,

	$$ ||D-D'||_{\textit{SD}} :=\frac{1}{2}\sum_{d\in \mathcal{X}}
  |\Pr[D=d] - \Pr[D'=d]| \leq \zeta.$$
  
  Neighboring databases are $\zt$-close distributions.
\end{defn}

For notion (2), we define neighboring samples as samples that differ in one entry.

\begin{defn} \citep{dpbook}
  Samples $Z$ and $Z'$ are neighboring if $z_i \neq z_i'$ for
  exactly one $i \in [n]$ (i.e. the Hamming distance between $Z$ and
  $Z'$ is 1).
\end{defn}
Alternatively we could define a finite sample as a multi-set, and use
the symmetric distance instead of the Hamming metric to measure
distance. Note that Definition 2.1 is a generalization of Definition
2.2.

For each of these notions, the definition of differential privacy
remains the same. That is, a randomized algorithm is private if
neighboring databases induce close distributions on its outcomes:

\begin{defn} 
  ($\eps$-differential privacy \citep{dworknoisesens}) A randomized algorithm
    $\A$ is $\eps$-differentially private if for all pairs of
    neighboring databases $D,D'$ and for all sets $\S$ of outputs,
    $$\Pr[\A(D)\in \S] \leq \exp(\eps)\Pr[\A(D')\in \S].$$
\end{defn}
\dk{need to talk about composition/post-processing theorems and/or exponenetial mechanism?}


\subsection{Exact Fairness}

In this setting our database notion is (1): a distribution $D$ over
$X$. This corresponds with the full distributional access setting of
[cite eq of opp hardt] where we have access to the joint probability
distribution $D = (X, A, Y)$ over $\X$, where $X$ are the features, $A$
sensitive attributes, and $Y$ true labels, respectively. Hence we state
our notion of exact fairness in these terms:

\begin{defn} 
  (Equal Opportunity \citep{hardteqop}). We say that a binary predictor $h$ satisfies
    equal opportunity with respect to $A$ and $Y$ if
    $$\Pr\{h = 1 | A =1, Y=1\} = \Pr\{h = 1 | A=0, Y=1\}.$$
\end{defn}

Note that this is the weaker of the two definitions of fairness in
Hardt et. al. however, it is sufficient to prove impossibility for this
notion since violating this notion implies violating the stronger
notion, equal odds. These measures of fairness belong to a broader
class of fairness constraints that is, constraints on functions of the
confusion matrix of a classifier (convex constraints paper).

\subsection{Approximate Fairness}
We now turn to the approximate fairness setting with and define
analogous definitions from the exact fairness setting. Here our notion
of a database is (2) i.e. we have access to a finite sample $Z$
consisting of entries drawn i.i.d. from an arbitrary distribution $D$
over $\X$.

For ease of notation we define $\zya := \{z_i \in Z | y_i = y, a_i = a \}$.

We also have an analogous definition for the group-conditional true
positive rates: \dk {clarify summation as such?: $\sum_{z_i \in \zya} h(x_i)$}
$$\gamma_{ya}^Z(h) = \frac{1}{|\zya|} \sum_{\zya} h(x).$$

\begin{defn} 
	($\alpha$-discrimination \citep{woodworthFollowUp}). We say that a binary
    predictor $h$ is $\alpha$-discriminatory with respect to a binary
    protected attribute $A$ on the population or on a sample $Z$ if,
    respectively:

$$\Gamma(h) := \max_{y\in \{0,1\}}|\g{y0} - \g{y1}| \leq \alpha ~~$$ {or}
$$\Gamma^Z(h) := \max_{y\in \{0,1\}}|\gz{y0} - \gz{y1}| \leq \alpha.$$

\end{defn}
When $\alpha = 0$ we are in the exact fairness setting. Note that by
\citep{woodworthFollowUp} the sample fairness measure $\Gamma^Z$ converges to the
population measure $\Gamma$ when $n$ is large. For the
analog of Definition 2.4 (Equality of Opportunity) where we only consider true positive
rates, the definition reduces to:

$$\Gamma^Z(h) := |\gz{10} - \gz{11}| \leq \alpha.$$

In the remainder of this paper, $\Gamma$ refers to this
fairness measure.

\dk {talk about motivation for approx fairness because of finite sample
(cannot achieve exact) and now also because of impossibility with privacy.}

\subsection{Preliminaries from learning theory}
\dk {do we need to factor a weight $\lambda$ into the new error
function? ie. $\ell + \lambda \Gamma$ talk in end of proof}
\dk {very not sure how to write this section, help?}
A concept is a function that labels examples taken from the domain $X$
by the elements of the range $Y$. A \emph{concept class} is a set of
concepts. The domain and range in $\cC$ are ensembles $X= \{X_d\}_{d\in
\N}$, $Y= \{Y_d\}_{d\in \N}$ where the representation size of the
elements in $X_d,Y_d$ is at most $d$. Since we focus on binary
classification problems, $d$ measures the size of examples in $X_d$.
When the size parameter is clear from the context or not important, we
omit the subscript $d$.

In our particular setting, our distribution of labeled examples is
arbitrary on $X_d \times Y_d$ and hence we consider the agnostic
setting that removes the realizability assumption. The goal of a
learner then is to output a hypothesis $h \in \H$ whose error with
respect to the distribution is close to the optimal possible by a
function from $\cC$. The misclassification error of $h$ on $D$ is
defined as:

$$err(h) = \Pr_{(x,y) \tilde D}[h(x) \neq y].$$

In our setting we consider the agnostic learnability setting.
\dk {not sure if following def is correct (wrt. concept class $\cC_d$ do
we consider $\cC$ in the agnostic setting?)}
\begin{defn}
	(Agnostic PAC Learning) A concept class $\cC$ over $\X$ is PAC
  learnable using hypothesis class $\H$ if there exists an algorithm
  $\A$ and a polynomial $poly(\cdot,\cdot)$ such that for all $d \in
  \mathbb{N}$, all concepts $c \in \mathcal{C}_d$, all distributions
  $D$ on $\X$ and all $\alpha,\beta \in (0,1/2)$, given inputs
  $\alpha,\beta,$ and $Z = (z_1, ... z_n)$, where n = $poly(d,
  1/\alpha, \log(1/\beta))$, $z_i = (x_i, a_i, y_i)$ drawn i.i.d. from
  D for $i \in [n]$, algorithm $\A$ outpus a hypothesis $h \in \H$
  satisfying
	$$\Pr[err(h) \leq OPT + \alpha] \geq 1-\beta.$$

\end{defn}
\dk {insert note about distribution-free learning}
\subsection{Private and Approximately Fair Agnostic Learning}
We define private and approximately fair PAC learners as algorithms that satisfy the definitions of differential privacy, approximate fairness (with high probability), and PAC learning.
\begin{defn}
	(Private and Approximately Fair Agnostic PAC Learning)
	Let $d, \alpha, \beta$ be as in Definition 2.6 (agnostic PAC
  learnigng) and $\eps > 0$. Concept class $\cC$ is (inefficiently)
  privately and approximately fair agnostically learnable using
  hypothesis class $\H$ if there exists an algorithm $\A$ that takes
  inputs $\eps, \alpha, \beta, Z$, where $|Z|=n$ is polynomial in
  $1/\eps, d, 1\alpha, \log(1/\beta)$ and satisfies
	\begin{enumerate}
		\item {[}Fairness{]} Algorithm $\A$ satisfies $\Pr[\Gamma(h) \leq
    OPT + \alpha] \geq 1-\beta$;
		\item {[}Privacy{]} For all $\eps>0$, algorithm $\A(\eps, \cdot, \cdot, \cdot)$ is $\eps$-differentially private;
		\item {[}Utility{]} Algorithm $\A$ agnostically PAC learns $\cC$ using $H$.
	\end{enumerate}
\end{defn}

\section{Achieving exact fairness and differential privacy is impossible}
We start with the full distributional access setting (which can be translated to the finite sample setting later). Let $\mathcal{X}$ be the data universe with elements $d_i = (x_i,a_i,y_i) \in \mathcal{X}$ where each entry consists respectively of the response, features, and protected attribute. Note that $x_i$ may have arbitrary correlation with $a_i$, including containing a copy of it.


\dk {actually we could do an impossibility result for
	any hard threshold for $\Gamma$ in any setting. Hence motivating approx
	fairness with high probability}

\begin{defn}
	For ease of notation we define
	$$\X_{ya} := \{d_i \in \X | y_i = y, a_i = a \}.$$
\end{defn}
\begin{defn}
	(non-trivial hypothesis class). We say that a hypothesis class $\H$ is  \emph{non-trivial} if there exists $d_1, d_0 \in \mathcal{X}$ such that for some $h \in \H$, $h(x_1) > h(x_0)$.
\end{defn}
\begin{lemma}Let $\mathcal{H}$ be a non-trivial hypothesis class. Then
  releasing an exactly fair hypothesis $h\in \mathcal{H}$ in a
  differentially private manner is impossible.
\end{lemma}
\dk{ (currently in weaker eq. of opp. fairness notion, can also be done
for stronger notions.) TODO: general proof of impossibility given a
`reasonable' fairness constraint on the confusion matrix? Seems like it
would be difficult since need to construct neighboring databases. Hence
TODO: proof for eq. odds? or say easy extension into other `reasonable'
fairness notions.}

\begin{proof}
	We will construct distributions $D, D'$ that are $\zt$-close and show impossibility. Suppose that the fair (as in
  Definition 2) hypothesis $h$ has a non-zero probability of being released by algorithm $\mathcal{A}$ on input of $D$. Let $d(\cdot)$ be the measure as
  induced by
  distribution $D$.

  Pick $d_1, d_0 \in \X_{11}$ such that $h(x_1) >
  h(x_0)$ and assume that $d_1, d_0 \in supp(D) \text{ and } supp(D')$.
  Let $\zt < \min_{i,D,D'}(d_{D}(d_i))$. $D'$ be a database with $\zt$
  more probability mass of $d_1$ and $\zt$ less mass of $d_0$. Then
  recalling that $d_i = (x_i, a_i,y_i)$:

$$\Pr_{D'}\{h(x) = 1 | a = 1, y=1\} = \int_{\X_{11}}h(x)dx +
\frac{\zt}{d(\X_{11})} h(x_1) - \frac{\zt}{d(\X_{11})} h(x_0) $$
$$= \Pr_{D}\{h(x) = 1 | a = 1, y=1\} + \frac{\zt}{d(\X_{11})} (h(x_1) - h(x_0))$$
$$>  \Pr_{D}\{h(x) = 1 | a = 1, y=1\}$$

where the last inequality is due to the assumption that $h(x_1) >
h(x_0)$.

Since we have not modified the distributions over $X_{10}$, $\Pr_{D'}\{h(x) = 1 | a = 1, y=1\} = \Pr_{D}\{h(x) = 1 | a = 1, y=1\}$.  Hence hypothesis $h$ is unfair (in the sense of definition 2.4) when evaluated on distribution $D'$
and cannot be released by the exactly fair algorithm $\A$ on input of
database $D'$. So

$$\Pr\{\mathcal{A}(D) = h\} \not\leq \exp(\eps)\Pr\{\mathcal{A}(D') = h\} = 0$$.

which violates the constraint of privacy in Definition 1.
\end{proof}

This result also applies to more general notions of statistical fairness. Definition 2.4, Equal Opportunity, is a strictly weaker notion of fairness than Equal Odds also defined in \dk{cite}. In other words, Equal Opportunity is a necessary condition for Equal Odds.

We can also use the same proof method to show impossibility of fairness and privacy
when we consider notions of approximate fairness such as disparate impact and mean
difference scores. Approximate in these notions of fairness meaning that the
resulting classifier (or anything released by the randomized algorithm) satisfies some hard threshold on the `level of discrimination' on the distribution.
One needs to construct neighboring distribution where when considering a hypothesis $h\in \H$, $h$ is fair on one distribution but not the other.
\dk {use `database' or `distribution' terminology?}


\begin{lemma}
  $(\eps, \zt)$-DP and fairness is impossible. \vg{need proof?}
\end{lemma}




\begin{corollary} $\Gamma(h) = |\Pr\{h(x) = 1 | y=1, a =0\} - \Pr\{h(x) = 1 | y = 1, a = 1\}|$ has sensitivity $\max(\frac{1}{\z{10}},\frac{1}{|Z_{11}|}, \frac{\gamma_{10}}{|\z{10}|-1} + \frac{\gamma_{11}}{|Z_{11}|+1}, \frac{\gamma_{10}}{|\z{10}|+1} + \frac{\gamma_{11}}{|Z_{11}|-1})$
\end{corollary}

\vg{Proof 1 is in the finite sample setting and Proof 2 is in the
distributional setting, not sure which of the two to keep. Finite
sample is easier/more applicable but the distributional is
general/converted easily to finite. We're leaning towards keeping
finite sample but aren't sure }

\dk{this proof seems like appendix material}

\dk{should we use the term "database" or "sample" for the finite sample setting?}
\begin{proof}
We examine two cases for neighboring databases $Z, Z'$:

Case 1: The difference in databases is in an entry within a subgroup
(i.e. within $Z_{1a}$ and $Z_{1a}'$ for $a \in \{0, 1\}$). WLOG let $a
= 0$. Let the differing entries be called $z \in \z{10}$ and $z' \in
Z'_{10}$. Then, we have

$$\gamma^{Z'}_{11} = \gamma^Z_{11}$$

but

$$\gamma_{10}^Z(h) = \frac{1}{|\z{10}|} \sum_{\z{10}} h(x,0)$$.
$$\gamma_{10}^{Z'}(h) = \frac{1}{|\z{10}|} (\sum_{z \in \z{10} \cup \z{10}'} h(x, 0) + h(x', 0)).$$

Then,

$$ \Gamma^{Z'}(h) = \gamma_{10}^{Z'}(h) - \gamma_{10}^{Z'}(h) $$
$$ = \gamma_{10}^{Z'}(h) - \gz{11} $$
$$ \leq \Gamma^{Z}(h) + \frac{1}{|\z{10}|} $$


Case 2: The neighboring databases are different in that an entry is added to one subgroup and another entry is removed from the other subgroup. Thus WLOG $Z' =(Z_{11}\setminus
\{z_1\} )\cup( \z{10}\setminus \{z_0\})$. Let $\gamma_{11}^{Z}(h)
<\gamma_{10}^{Z}(h)$ then

$$\gamma_{11}^{Z'}(h) = \frac{1}{|Z'_{1a}|} \sum_{Z'_{1a}} h(x)$$
$$= \frac{1}{|Z'_{11}|} (\sum_{Z_{11}} h(x)-h(x_1)) = \frac{1}{|Z_{11}|-1} (\sum_{Z_{11}} h(x)-h(x_1))$$
Similarly,
$$\gamma_{10}^{Z'}(h) = \frac{1}{|Z'_{10}|} \sum_{Z'_{10}} h(x)$$
$$ = \frac{1}{|Z'_{10}|} (\sum_{\z{10}} h(x)+h(z_0))= \frac{1}{|\z{10}|+1} (\sum_{\z{10}} h(x)+h(z_0))$$

Hence with notation $ \Gamma^Z = \Gamma^Z(h)$ for clarity, $$|\Gamma^{Z'}- \Gamma^{Z}| = (\gamma_{10}^{Z'} - \gamma_{11}^{Z'}) - (\gamma_{10}^{Z} - \gamma_{11}^{Z})$$


$$=(\gamma_{10}^{Z'}- \gamma_{10}^{Z}) + (\gamma_{11}^{Z} -\gamma_{11}^{Z'})$$
Let $\zt_{10}= \gamma_{10}^{Z'}- \gamma_{10}^{Z}$ and  $\zt_{11} = \gamma_{11}^{Z} -\gamma_{11}^{Z'}$

$$\zt_{10} = \frac{1}{|\z{10}|+1} (\sum_{\z{10}} h(x)+h(z_0)) - \gamma_{10}^{Z}$$
$$\zt_{11} = \gamma_{11}^{Z}- \frac{1}{|Z_{11}|-1} (\sum_{Z_{11}} h(x)-h(x_1))$$
\dk {insert rest of proof, easy algebra}

$$|\Gamma^{Z'}- \Gamma^{Z}| = \zt_{10} + \zt_{11}= \frac{\gamma_{10}}{|\z{10}|-1} + \frac{\gamma_{11}}{|Z_{11}|+1} $$
\end{proof}

\begin{proof}
Construct $\zt$-close neighboring databases $D$ and $D'$ such that

$$\Pr[D'=d_{10}] - \Pr[D=d_{10}] = \zt$$
$$\Pr[D=d_{11}] - \Pr[D'=d_{11}] = \zt$$



and assume that $d_0, d_1 \in \text{supp}(D')$ and $\in \text{supp}(D)$. Let $d_{10} \in D_{10}$ and $d_{11} \in D_{11}$ and WLOG $\gamma_{10}^Z(h) > \gamma_{11}^Z(h)$ then,

$$\Gamma^{D'}(h) = \gamma_{10}^{D'}(h) - \gamma_{11}^{D'}(h)$$
$$= \int_{D'_{10}}h(x)dx - \int_{D'_{11}}h(x)dx$$
$$= \int_{D_{10}}h(x)dx + \frac{\zt}{dD_{10}}h(x_{10}) - \int_{D'_{11}}h(x)dx - \frac{\zt}{dD_{11}}h(x_{11})$$
$$\leq \Gamma^{D}(h) + \frac{\zt}{dD_{10}} + \frac{\zt}{dD_{11}}$$

where $dD_{1a}$ is the measure of the set $D_{1a},$ for $a \in \{0,1\}$. Hence,
$$\zt\Gamma = \frac{\zt}{dD_{10}} + \frac{\zt}{dD_{11}}.$$
\end{proof}

\dk{Q: more elegant/complete proof? right now it seems we are doing by cases. ie. $\zt$ close changes $\gamma_{ya}(h)$ by at most ... therefore ..}

\section{Approximate fairness with differentially privacy}



We now turn to the finite sample setting where we release a hypothesis
that minimizes the training error. Our goal is now approximate
fairness. We use the exponential mechanism in the same way as it is
used in private PAC learning. Defining the sample as $Z$, $|Z| = n$ and
$\Gamma^{Z}$ as our in-sample fairness measure, we give the algorithm:
$$\mathcal{A}^\eps : \text{Output hypothesis }h \in \mathcal{H} \text{
with probability proportional to }$$
\begin{align}
\exp(-\frac{\eps \cdot u(Z,h)}{2\Delta u})
\end{align}

where

$$u(Z,h) = \Gamma^Z(h) + \ell^Z(h)$$.
$$\Delta u(Z,h) = \Delta\ell + \Delta{\Gamma} \approx O(1/n + 1/|\z{10}|+1/|Z_{11}|)$$

$$\ell^Z(h) = \frac{1}{n} \sum_{(x,y) \in Z}\Pr[h(x) \neq y]$$



This algorithm is the exponential mechanism in McSherry and Talwar, and so it is differentially private.

\begin{lemma}
  The algorithm $\A_\eps$ is $\eps$-differentially private.
\end{lemma}

Note that except for when $|\H|$ is polynomial, the exponential
mechanism does not necessarily yield a polynomial time algorithm.


\begin{theorem}
	(Generic private fair learner) For all $d \in \mathbb{N}$, any
  concept class $\cC_d$ whose cardinality is at most
  $\exp(\text{poly}(d))$ is privately and approximately fairly
  agnostically learnable using $\H_d = \cC_d$. More precisely, the
  learner uses $n = ..$ labeled examples from $D$, where $\eps,
  \alpha$, and $\beta$ are parameters of the private learning.
  \dk {need to be sure of constants before calculating n. Will be poly for sure}
\end{theorem}

\begin{proof}
	Let $\A_{\eps}$ be as defined above. The privacy condition is
  satisfied by Lemma.

	Now we show that the utility condition is also satisfied. Let the
  event $E = \{\A_{\eps} = h \text{ with } u(h) > OPT + \alpha\}$. We
  need that $\Pr[E] \leq \beta$. We define the utility of $h$ as

	$$u(Z,h) = \Gamma^Z(h)+ \ell^Z(h)$$.
\dk {introduce this before proof and have a discussion about it? OR discussion after proof about utility and implication of $\alpha$ accuracy/fairness whp.}

	Recall that $Z$ is the sample drawn i.i.d. from a distribution $D$.
  By Chernoff-Hoeffding bounds (insert appendix ref. see below proof
  for now),

	$$\Pr[|u(Z,h) - u(D,h)| \geq \rho] \leq 4\exp(\frac{-\rho^2n}{2})$$

	for all hypotheses $h \in \H_d$. Hence,

	$$\Pr[|u(Z,h) - u(D,h)| \geq \rho \text{ for some } h \in \H_d] \leq 4|\H_d|\exp(\frac{-\rho^2n}{2})$$

	\dk{need following proof? relatively similar to kasiviswanathan what
  can we learn privately, except we have different utility and sensitivity}
	Now we analyze $\A_\eps(Z)$ conditioned on the event that for all
  $h\in \H_d$, $|u(Z,h) - u(D,h)| < \rho$. For every $h \in \H_d$, $\Pr[\A_\eps(Z) = h]$ is

	$$\frac{\exp(-\frac{\eps}{2\Delta u} \cdot
  u(Z,h))}{\sum_{h'\in\H_d}\exp(-\frac{\eps}{2\Delta u} \cdot u(Z,h'))}
  \leq \frac{\exp(-\frac{\eps}{2\Delta u} \cdot
  u(Z,h))}{\max_{h'\in\H_d}\exp(-\frac{\eps}{2\Delta u} \cdot u(Z,h'))} $$
	$$= \exp(-\frac{\eps}{2\Delta u}(u(Z,h) - \min_{h'\in\H_d}u(Z,h')))$$
	$$\leq \exp(-\frac{\eps}{2\Delta u}(u(Z,h) - (OPT + \rho)))$$

	Hence the probability that $\A_\eps(Z)$ outputs a hypothesis $h \in
  \H_d$ such that $u(Z,h) > OPT + 2\rho$ is at most
  $|\H_d|\exp(-\frac{\eps\cdot\rho}{2\Delta u})$

	Setting $\rho = \alpha/3$. If $u(D,h) \geq OPT + \alpha$ then
  $|u(D,h) - u(Z,h)| \geq \alpha/3$ or $u(Z,h) \geq OPT + 2\alpha/3$.
  Hence

	$$\Pr[E] \leq |\H_d|(4\exp(\frac{-\alpha^2n}{18}) + \exp(-\frac{\eps\cdot\alpha}{6\Delta u})) \leq \beta$$.

	Where the inequality holds for $n \geq $.
\end{proof}
{\bf Theorem} (Real-valued Additive Chernoff-Hoeffding Bound). Let
$X_1,...,X_d$ be i.i.d. random variables with $\mathbb{E}[X_i] = \mu$
and $a \leq X_i \leq b$ for all $i$. Then for every $\rho > 0$,

$$Pr[|\frac{\sum_i X_i}{n} - \mu| > \rho] \leq 2\exp(\frac{-2\rho^2n}{(b-a)^2})$$

\dk {pretty sure the constants are correct and the union bound is necessary}
Chernoff bounds for $u(Z,h)$:

Let $$X_i^a = n(\frac{\1_{z\in Z_{1a}}h(x)}{|Z_{1a}|} - \frac{\1_{z\in
Z_{1\neg a}}h(x)}{|Z_{1 \neg a}|} ) + \Pr[h(x) \neq y]$$

Then $$\max_{a\in \{0,1\}}\frac{1}{n} \sum_Z X_i^a$$
$$ =\frac{1}{n}
\max_{a\in \{0,1\}} \sum_Z (n(\frac{\1_{z\in Z_{1a}}h(x)}{|Z_{1a}|} -
\frac{\1_{z\in Z_{1\neg a}}h(x)}{|Z_{1 \neg a}|} ) + \Pr[h(x) \neq y])$$

$$= |\sum_Z \frac{\1_{z\in \z{10}}h(x)}{|\z{10}|} - \frac{\1_{z\in
Z_{11}}h(x)}{|Z_{11}|}| +  \sum_Z \frac{\Pr[h(x) \neq y] }{n}$$

$$= |\frac{1}{|\z{10}|} \sum_{z\in \z{10}} h(x) - \frac{1}{|Z_{11}|} \sum_{z\in Z_{11}} h(x)| +  \sum_Z \frac{\Pr[h(x) \neq y] }{n}$$

$$=|\gamma_{10}^Z - \gamma_{11}^Z| + \ell^Z(h) $$
$$=||(\Gamma^Z(h), \ell^Z(h)||_{1} = u(Z,h)$$

Hence

$$\Pr[|\frac{1}{n} \sum_Z X_i^a - \mathbb{E}[X_i^a]| > \rho] \leq 2\exp(\frac{-2\rho^2n}{(2-(-1))^2})$$

By union bound (over the choice of $a \in \{0,1\}$

$$\Pr[|u(Z,h) - u(D,h)| > \rho] \leq 4\exp(\frac{-2\rho^2n}{9})$$

\dk {Note: can do extension into other definitions of fairness (with constant factor).
	Can also extend into a different norm for $u(Z,h)$ if we have a
  concentration of measure theorem for the different norm. There are
  also random matrix concentration bounds (can do concentration for all
  functions of the confusion matrix?)}

\section{A polynomial time algorithm for approximately fair and private classification}
\dk {we've found a promising method: kamishima et al. working on proving DP and figuring out code. kamishima provides their original data in their repo too !}

%\section{Appendix}
%p-norm discussion from exp mech?
