<<<<<<< HEAD
% !TEX root = main.tex

\section{Introduction}
Recent applications of machine learning in human-relevant domains
raise concerns that too much of some individual's information might be
leaked through a model learned on some training data. For example, if
one learns a model based on historical health data, many algorithms
have some real possibility of outputting a model containing
information about individuals' HIV status or other sensitive
information.  Resultingly, academia and industry have spent much
effort designing and implementing \emph{differentially private}
machine learning methods.

Differential privacy gives a strong guarantee to individuals whose
data we use to train a model; these human-centric uses of ML systems
have also raised concerns of equity of the predictive power for a
model on different populations. The \emph{fairness} of a model can be
thought of as some equitable performance guarantee for individuals who
will be evaluated by the model, rather than a guarantee for someone
participating in the model's training process. When phrasing privacy
and fairness of a model this way, a natural question arises: in what
settings can we learn a model which is private in the training data
while guaranteeing equitable performance for multiple populations on
which the model will ultimately run? More formally, will it be
possible to guarantee privacy of training data, fairness for the
predictions made on two populations, and some accuracy overall?

\jm{More stuff here}

\subsection{Related Work}

The focus on fairness in machine learning and its relationship to
differential privacy was explored in early work in the community \citep{dworkfta}. This work introduced the concept of
treating similar people similarly, where ``similarity'' is defined as
some task-specific metric over individuals.  The authors then point
out that this desiderata can be formulated as a Lipschitz constraint
and show how to satisfy it using tools from DP.

More recently, \citet{ekstrandprivfair} raised questions of whether
statistical notions of equitable predictive power, such as equalized
odds \cite{hardteqop}, are compatible with privacy. In a limiting sense,
when one talks about feature and model selection, there appears to be
some tension here: an additional feature might increase the possible
privacy loss an individual faces, while the additional feature should
only make EO easier to satisfy (as alluded to in Hanna's paper on
exploration and exploitation). However, to the best of the authors'
knowledge, no work has heretofore shown whether an ML algorithm can
guarantee individuals differential privacy and their populations some
kind of group fairness simultaneously.

The technical tools we use for this work come from differential price
vacy, using the exponential mechanism \cite{2dplimits}, add gerrymandering stuff.



Related work we definitely want to cover: fairness through awareness,
the paper proposing we study privacy + fairness, dwork's ``it isn't
private and it isn't fair'' or whatever...

\subsection{Our Contributions}
We present two contributions to the study of the intersection of
differential privacy and fairness in the binary classification setting.
First, we show that it is impossible to achieve both differential
privacy and exact fairness even in the setting where we have access to
the full distribution of the data. We then consider a notion of
approximate fairness in the finite sample access setting and show that
there exists a private PAC learner that is differentially private and
satisfies approximate fairness with high probability. \vg{Finally, we
present a polynomial time algorithm to achieve the differentially
private and approximately fair scheme outlined above and show its
empirical performance on certain datasets.}
=======
% !TEX root = main.tex

\section{Introduction}
Recent applications of machine learning in human-relevant domains
raise concerns that too much of some individual's information might be
leaked through a model learned on some training data. For example, if
one learns a model based on historical health data, many algorithms
have some real possibility of outputting a model containing
information about individuals' HIV status or other sensitive
information.  Resultingly, academia and industry have spent much
effort designing and implementing \emph{differentially private}
machine learning methods.

Differential privacy gives a strong guarantee to individuals whose
data we use to train a model; these human-centric uses of ML systems
have also raised concerns of equity of the predictive power for a
model on different populations. The \emph{fairness} of a model can be
thought of as some equitable performance guarantee for individuals who
will be evaluated by the model, rather than a guarantee for someone
participating in the model's training process. When phrasing privacy
and fairness of a model this way, a natural question arises: in what
settings can we learn a model which is private in the training data
while guaranteeing equitable performance for multiple populations on
which the model will ultimately run? More formally, will it be
possible to guarantee privacy of training data, fairness for the
predictions made on two populations, and some accuracy overall?

\jm{More stuff here}

\subsection{Related Work}

The focus on fairness in machine learning and its relationship to
differential privacy was explored in early work in the community \citep{dworkfta}. This work introduced the concept of
treating similar people similarly, where ``similarity'' is defined as
some task-specific metric over individuals.  The authors then point
out that this desiderata can be formulated as a Lipschitz constraint
and show how to satisfy it using tools from DP.

More recently, \citet{ekstrandprivfair} raised questions of whether
statistical notions of equitable predictive power, such as equalized
odds \cite{hardteqop}, are compatible with privacy. In a limiting sense,
when one talks about feature and model selection, there appears to be
some tension here: an additional feature might increase the possible
privacy loss an individual faces, while the additional feature should
only make EO easier to satisfy (as alluded to in Hanna's paper on
exploration and exploitation). However, to the best of the authors'
knowledge, no work has heretofore shown whether an ML algorithm can
guarantee individuals differential privacy and their populations some
kind of group fairness simultaneously.

The technical tools we use for this work come from differential price
vacy, using the exponential mechanism \cite{2dplimits}, add gerrymandering stuff.



Related work we definitely want to cover: fairness through awareness,
the paper proposing we study privacy + fairness, dwork's ``it isn't
private and it isn't fair'' or whatever...

\subsection{Our Contributions}
We present two contributions to the study of the intersection of
differential privacy and fairness in the binary classification setting.
First, we show that it is impossible to achieve both differential
privacy and exact fairness even in the setting where we have access to
the full distribution of the data. We then consider a notion of
approximate fairness in the finite sample access setting and show that
there exists a private PAC learner that is differentially private and
satisfies approximate fairness with high probability. \vg{Finally, we
present a polynomial time algorithm to achieve the differentially
private and approximately fair scheme outlined above and show its
empirical performance on certain datasets.}
>>>>>>> 5a4a8821fa04d333d8fffcf0456a9148244448d3
