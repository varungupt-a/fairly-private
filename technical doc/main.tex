\documentclass[runningheads]{article}
\usepackage{amsmath,amsthm,amssymb,amsfonts}
\usepackage{mathtools}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{fancyhdr}
\usepackage{hyperref}
\usepackage{natbib}
\usepackage{bbm}
\usepackage{algorithm}
\newcommand{\dk}[1]{\textcolor{red}{[DK: #1]}}


\newcommand\tab[1][0.5cm]{\hspace*{#1}}
\newcommand{\I}{\mathcal{I}}
\newcommand{\M}{\mathcal{M}}
\newcommand{\A}{\mathcal{A}}
\renewcommand{\H}{\mathcal{H}}
\newcommand{\C}{\mathcal{C}}
\newcommand{\1}{\mathbbm{1}}


\renewcommand{\S}{\mathcal{S}}
\newcommand{\del}{\delta}
%
\theoremstyle{definition}
\newtheorem{defn}{Definition}[section]

%
\newenvironment{claim}[1]{\par\noindent\underline{Claim:}\space#1}{}
\newenvironment{claimproof}[1]{\par\noindent\underline{Proof:}\space#1}

\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=blue,
}

\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}
\DeclarePairedDelimiter{\floor}{\lfloor}{\rfloor}

\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}


\newenvironment{problem}[2][Problem]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
%If you want to title your bold things something different just make another thing exactly like this but replace "problem" with the name of the thing you want, like theorem or lemma or whatever
 
\begin{document}

%\renewcommand{\qedsymbol}{\filledbox}
%Good resources for looking up how to do stuff:
%Binary operators: http://www.access2science.com/latex/Binary.html
%General help: http://en.wikibooks.org/wiki/LaTeX/Mathematics
%Or just google stuff
 
\title{Privacy $\cap$ Fairness}
\author{Rachel Cummings, Varun Gupta, Dhamma Kimpara, Jamie Morgenstern}
\maketitle


\section{Preliminaries}

% learnability A concept class $\mathcal{C}$ over $\X$ is PAC learnable using hypothesis class $\H$ if there exists an algorithm $\A$ and a polynomial $poly(\dot,\dot)$ such that for all $d \in \mathbb{N}$, all concepts $c \in \mathcal{C}_d$, all distributions $D$ on $X_d$ and all $\alpha,\beta \in (0,1/2)$, given inputs $\alpha,\beta,$ and $Z = (z_1, ... z_n)$, where n = $poly(d, 1/\alpha, log(1/\beta)), ... kasivi ref.\\



\section{Achieving exact fairness and differential privacy is impossible}
We start with the the continuous setting (which can be translated to the discrete sample setting later). Let $\mathcal{X}$ be the data universe with elements $d_i = (y_i,x_i,a_i) \in \mathcal{X}$ where each entry consists respectively of the response, features, and protected attribute. Note that $x_i$ may also contain $a_i$ to be used by a hypothesis $h$. We define a neighboring database as defined in [insert DP paper here by Talwar]:

{\bf Definition 1} ($\epsilon$-differential privacy). A randomized algorithm $\A$ is $\epsilon$-differentially private if for all neighboring databases $D, D'$, and for all sets $\S$ of outputs, 
$$\Pr[\A(D)\in \S] \leq \exp(\epsilon)\Pr[\A(D')\in \S]$$
The probability is taken over the random coins of $\A$.\\
{\bf Definition 2} (Equal Opportunity). We say that a binary predictor $h$ satisfies equal opportunity with respect to $A$ and $Y$ if (need explicit form of database
$$\Pr\{h(x) = 1 | a = 1, y = 1\} = \Pr\{h(x) = 1 | a=0, y=1\}$$ 


{\bf Definition 3} (Statistical distance and $\delta$-closeness). Databases  $D$ and $D'$ (distributions over $\mathcal{X}$) are $\delta$-close if the statistical distance between their distributions is at most $\delta$, i.e.,

$$ ||D-D'||_{\textit{SD}} :=\frac{1}{2}\sum_{x\in \mathcal{X}} |\Pr[D=x] - \Pr[D'=x]| \leq \delta$$

{\bf Definition 4?} For ease of notation we define
$$D_{ya} := \{d_i \in D | y_i = y, a_i = a \}$$

{\bf Definition 5} Non-trivial hypothesis class. We say that a hypothesis class $\H$ is non-trivial if there exists $d_1, d_0 \in \mathcal{X}$ such that for some $h \in \H$, $h(x_1) > h(x_0)$.

{\bf Lemma 1} Let $\mathcal{H}$ be a non-trivial hypothesis class. Releasing an exactly fair hypothesis $h\in \mathcal{H}$ in a differentially private manner is impossible.

\dk{ (currently in weaker eq. of opp. fairness notion, can also be done for stronger notions.) TODO: general proof of impossibility given a `reasonable' fairness constraint on the confusion matrix? Seems like it would be difficult since need to construct neighboring databases. Hence TODO: proof for eq. odds? or say easy extension into other `reasonable' fairness notions.}

{\bf Proof} Let $D, D'$ be $\delta$-close and suppose that the fair (as in definition 2) hypotheses $h$ is released by algorithm $\mathcal{A}$ on input of $D$. Let $d(\cdot)$ be the measure as defined by the distribution $D$. Pick $d_1, d_0 \in D_{11}$ such that $h(x_1) > h(x_0)$ and assume that $d_1, d_0 \in supp(D) \text{ and } supp(D')$. Let $\delta < \min_{i,D,D'}(d_{D}(d_i))$ $D'$ be a database with $\delta$ more probability mass of $d_1$ and $\delta$ less mass of $d_0$. Then recalling that $d_i = (y_i, x_i, a_i)$:

$$\Pr_{D'}\{h(x) = 1 | a = 1, y=1\} = \int_{D_{11}}h(x)dx + \frac{\delta}{d(D_{11})} h(x_1) - \frac{\delta}{d(D_{11})} h(x_0) $$
$$= \Pr_{D}\{h(x) = 1 | a = 1, y=1\} + \frac{\delta}{d(D_{11})} (h(x_1) - h(x_0))$$
$$>  \Pr_{D}\{h(x) = 1 | a = 1, y=1\}$$

where the last inequality is due to the assumption that $h(x_1) > h(x_0)$. Hence hypothesis $h$ is unfair when applied to database $D'$ and cannot be released by the exactly fair mechanism $\A$ on input of database $D'$. So

$$\Pr\{\mathcal{A}(D) = h\} \not\leq \exp(\epsilon)\Pr\{\mathcal{A}(D') = h\} = 0$$.

Which violates the definition of privacy in definition 1. $\square$ 
\vspace{0.5cm}

{\bf Lemma 2} $(\epsilon, \delta)$-DP and fairness is impossible. Need proof


\subsection{Defining approximate fairness}
We now turn to the approximate fairness and define analogs of the definitions in the exact fairness setting. 

Define $Z_{ya} := \{z_i \in Z | y_i = y, a_i = a \}$ and

$$\gamma_{ya}^Z(h) = \frac{1}{|Z_{ya}|} \sum_{Z_{ya}} h(x,a)$$


{\bf Definition 6} ($\alpha$-discrimination [srebro ref]). We say that a binary predictor $h$ is $\alpha$-discriminatory with respect to a binary protected attribute $A$ on a sample $Z$ if (need explicit form of database).

$$\Gamma^Z(h) := \max_{y\in \{0,1\}}|\gamma_{y0}^Z(h) - \gamma_{y1}^Z(h)| \leq \alpha$$


When $\alpha = 0$ we are in the exact fairness setting. To achieve an approximate analog to equality of opportunity in definition 2. We define:

$$\Gamma^Z(h) := |\gamma_{10}^Z(h) - \gamma_{11}^Z(h)| \leq \alpha$$

We now focus only on the equality of opportunity setting.

{\bf Corollary 1} $\Gamma(h) = |\Pr\{h(x) = 1 | y=1, a =0\} - \Pr\{h(x) = 1 | y = 1, a = 1\}|$ has sensitivity 

{\bf Proof} 

Construct $\delta$-close neighboring databases $D$ and $D'$ such that

$$\Pr[D'=d_{10}] - \Pr[D=d_{10}] = \delta$$
$$\Pr[D=d_{11}] - \Pr[D'=d_{11}] = \delta$$



and assume that $d_0, d_1 \in \text{supp}(D')$ and $\in \text{supp}(D)$. Let $d_{10} \in D_{10}$ and $d_{11} \in D_{11}$ and WLOG $\gamma_{10}^Z(h) > \gamma_{11}^Z(h)$ then,

$$\Gamma^{D'}(h) = \gamma_{10}^{D'}(h) - \gamma_{11}^{D'}(h)$$
$$= \int_{D'_{10}}h(x)dx - \int_{D'_{11}}h(x)dx$$
$$= \int_{D_{10}}h(x)dx + \frac{\delta}{dD_{10}}h(x_{10}) - \int_{D'_{11}}h(x)dx - \frac{\delta}{dD_{11}}h(x_{11})$$
$$\leq \Gamma^{D}(h) + \frac{\delta}{dD_{10}} + \frac{\delta}{dD_{11}}$$

where $dD_{1a}$ is the measure of the set $D_{1a}, a \in \{0,1\}$. Hence
$$\Delta\Gamma = \frac{\delta}{dD_{10}} + \frac{\delta}{dD_{11}}$$

\dk{Q: more elegant/complete proof? right now it seems we are doing by cases. ie. $\delta$ close changes $\gamma_{ya}(h)$ by at most ... therefore ..
Q: should we do this proof in finite sample context or just translate this to the finite sample (with $\delta = 1/n$)}

\section{Approximate fairness with differentially privacy}



We now turn to the finite sample setting where we release a hypothesis that minimizes the training error. Our goal is now approximate fairness. We use the exponential mechanism in the same way as it is used in private PAC learning. Defining the sample as $Z$, $|Z| = n$ and $\Gamma^{Z}$ as our in-sample fairness measure, we give the algorithm:
$$\mathcal{A}^\epsilon : \text{Output hypothesis }h \in \mathcal{H} \text{ with probability proportional to }$$
\begin{align}
\exp(-\frac{\epsilon \cdot u(Z,h)}{2\Delta u})
\end{align}

where

$$u(Z,h) = ||(\Gamma^Z(h), \ell^Z(h)||_{1}$$.
$$\Delta u(Z,h) = ||(\Delta\ell,\Delta{\Gamma})||_a \approx O(||1/n,1/|Z_{10}|+1/|Z_{11}|||_1)$$

$$\ell^Z(h) = \frac{1}{n} \sum_{(x,y) \in Z}\Pr[h(x) \neq y]$$



This algorithm is the exponential mechanism in McSherry and Talwar, and so it is differentially private. 

{\bf Lemma 2} The algorithm $\A_\epsilon$ is $\epsilon$-differentially private.
Need proof? 

Note that except for when $|\H|$ is polynomial, the exponential mechanism does not necessarily yield a polynomial time algorithm.

{\bf Theorem 1}(Generic private fair learner) For all $d \in \mathbb{N}$, any concept class $\mathcal{C}_d$ whose cardinality is at most $\exp(\text{poly}(d))$ is privately fairly agnostically learnable using $\H_d = \C_d$. More precisely, the learner uses $n = ..$ labeled examples from $D$, where $\epsilon, \alpha$, and $\beta$ are parameters of the private learning.
%\dk{Q: ``the domain and range of the concepts in $\mathcal{C}$ is understood to be ensembles $X = \{X_d\}_{d\in \mathbb{N}}$ and $Y = \{Y_d\}_{d\in \mathbb{N}}$, where the representation of elements $X_d, Y_d$ is of size at most $d$''}

{\it Proof.} Let $\A_{\epsilon}$ be as defined above. The privacy condition is satisfied by Lemma.

Now we show that the utility condition is also satisfied. Let the event $E = \{\A_{\epsilon} = h \text{ with } u(h) > OPT + \alpha\}$. We need that $\Pr[E] \leq \beta$. We define the utility of $h$ as

$$u(Z,h) = ||(\Gamma^Z(h), \ell^Z(h)||_{p}$$.

By Chernoff-Hoeffding bounds (insert appendix ref. see below proof for now),



$$\Pr[|u(Z,h) - u(D,h)| \geq \rho] \leq 4\exp(\frac{-\rho^2n}{2})$$

for all hypotheses $h \in \H_d$. Hence, 

$$\Pr[|u(Z,h) - u(D,h)| \geq \rho \text{ for some } h \in \H_d] \leq 4|\H_d|\exp(\frac{-\rho^2n}{2})$$

\dk{need following proof? relatively similar to kasiviswanathan what can we learn privately}
Now we analyze $\A_\epsilon(Z)$ conditioned on the event that for all $h\in \H_d$, $|u(Z,h) - u(D,h)| < \rho$. For every $h \in \H_d$, $\Pr[\A_\epsilon(Z) = h]$ is 

$$\frac{\exp(-\frac{\epsilon}{2\Delta u} \cdot u(Z,h))}{\sum_{h'\in\H_d}\exp(-\frac{\epsilon}{2\Delta u} \cdot u(Z,h'))} \leq \frac{\exp(-\frac{\epsilon}{2\Delta u} \cdot u(Z,h))}{\max_{h'\in\H_d}\exp(-\frac{\epsilon}{2\Delta u} \cdot u(Z,h'))} $$
$$= \exp(-\frac{\epsilon}{2\Delta u}(u(Z,h) - \min_{h'\in\H_d}u(Z,h')))$$
$$\leq \exp(-\frac{\epsilon}{2\Delta u}(u(Z,h) - (OPT + \rho)))$$

Hence the probability that $\A_\epsilon(Z)$ outputs a hypothesis $h \in \H_d$ such that $u(Z,h) > OPT + 2\rho$ is at most $|\H_d|\exp(-\frac{\epsilon\cdot\rho}{2\Delta u})$

Setting $\rho = \alpha/3$. If $u(D,h) \geq OPT + \alpha$ then $|u(D,h) - u(Z,h)| \geq \alpha/3$ or $u(Z,h) \geq OPT + 2\alpha/3$. Hence

$$\Pr[E] \leq |\H_d|(4\exp(\frac{-\alpha^2n}{18}) + \exp(-\frac{\epsilon\cdot\alpha}{6\Delta u})) \leq \beta$$.

Where the inequality holds for $n \geq $. $\square$ 

{\bf Theorem} (Real-valued Additive Chernoff-Hoeffding Bound). Let $X_1,...,X_d$ be i.i.d. random variables with $\mathbb{E}[X_i] = \mu$ and $a \leq X_i \leq b$ for all $i$. Then for every $\rho > 0$,

$$Pr[|\frac{\sum_i X_i}{n} - \mu| > \rho] \leq 2\exp(\frac{-2\rho^2n}{(b-a)^2})$$

\dk {needs checking}
Chernoff bounds for $u(Z,h)$:

Let $$X_i^a = n(\frac{\1_{z\in Z_{1a}}h(x)}{|Z_{1a}|} - \frac{\1_{z\in Z_{1\neg a}}h(x)}{|Z_{1 \neg a}|} ) + \Pr[h(x) \neq y]$$

Then $$\max_{a\in \{0,1\}}\frac{1}{n} \sum_Z X_i^a =\frac{1}{n}  \max_{a\in \{0,1\}} \sum_Z n(\frac{\1_{z\in Z_{1a}}h(x)}{|Z_{1a}|} - \frac{\1_{z\in Z_{1\neg a}}h(x)}{|Z_{1 \neg a}|} ) + \Pr[h(x) \neq y]$$

$$= |\sum_Z \frac{\1_{z\in Z_{10}}h(x)}{|Z_{10}|} - \frac{\1_{z\in Z_{11}}h(x)}{|Z_{11}|}| +  \sum_Z \frac{\Pr[h(x) \neq y] }{n}$$

$$= |\frac{1}{|Z_{10}|} \sum_{z\in Z_{10}} h(x) - \frac{1}{|Z_{11}|} \sum_{z\in Z_{11}} h(x)| +  \sum_Z \frac{\Pr[h(x) \neq y] }{n}$$

$$=|\gamma_{10}^Z - \gamma_{11}^Z| + \ell^Z(h) $$
$$=||(\Gamma^Z(h), \ell^Z(h)||_{1} = u(Z,h)$$

Hence

$$\Pr[|\frac{1}{n} \sum_Z X_i^a - \mathbb{E}[X_i^a]| > \rho] \leq 2\exp(\frac{-2\rho^2n}{(2-0)^2})$$

By union bound (over the choice of $a \in \{0,1\}$

$$\Pr[|u(Z,h) - u(D,h)| > \rho] \leq 4\exp(\frac{-\rho^2n}{2})$$

\dk {Note: can do extension into other definitions of fairness (with constant factor).
Can also extend into a different norm for $u(Z,h)$ if we have a concentration of measure theorem for the different norm. There are also random matrix concentration bounds (can do concentration for all functions of the confusion matrix?)}

\section{Approximately fair correction with differentially private linear programs}
Do DP-LP

\section{A polynomial time algorithm for approximately fair and private classification}
\clearpage

Old stuff:

Our initial approach was to look at this algorithm that achieves equality of opportunity or equal odds by flipping (with some probability) the label of the original non-fair algorithm, $\hat{Y}$, depending on the output label and the class membership $A$. $\hat{Y}$ is trained on $(X,Y)$, the features $X$ and the true label $Y$. When we flip labels given by $\hat{Y}$ we assume that we are given full access to the distribution $(Y,A,\hat{Y})$.

Essentially, modifying $\hat{Y}$ we create a new classifier $\tilde{Y}$ that is fair by setting the probabilities $\text{Pr}\{\tilde{Y} = 1 | \hat{Y} = \hat{y}, A = a \}$ for $\hat{y}, a \in \{0,1\}$ appropriately. The ``pipeline'' with the available information at each stage laid out plainly is as follows:

$$(X,Y) \xrightarrow[]{\text{vanilla training}} (\hat{Y},A,Y) \xrightarrow[]{\text{fairness}} \tilde{Y} $$


\section{Privatizing Woodworth et. al. (2017)}
(This is the follow-up to Eq. of Opp.)

\subsection{Brief summary}
This paper gives a two step algorithm in a learning model with finite samples and hypothesis classes. They split the training set into two parts. The first step is traditional ERM (with 0-1 loss) but with a sample based fairness constraint using half of the training set. The second step takes the outputted hypothesis from step 1 and does the same post-processing step as in the algorithm of Equality of Opportunity.


\section{Privatizing first}
With the finite sample setting and also the case that we cannot achieve exact fairness privately, this line of inquiry now seems to be fruitful. It would not take long to analyze the effect of first privatizing via synthetic data or adding noise to the training data (smallDB etc.) and then training a fair classifier. 

\section{Appendix}
p-norm discussion from exp mech?



\end{document}